                                               Practical and Asymptotically Optimal Quantization of
                                           High-Dimensional Vectors in Euclidean Space for Approximate
                                                              Nearest Neighbor Search
                                                              Jianyang Gao                                                   Yutong Gou                                    Yuexuan Xu
                                               Nanyang Technological University                              Nanyang Technological University                  Nanyang Technological University
                                                         Singapore                                                     Singapore                                         Singapore
                                                  jianyang.gao@ntu.edu.sg                                        yutong003@e.ntu.edu.sg                           yuexuan001@e.ntu.edu.sg
arXiv:2409.09913v1 [cs.DB] 16 Sep 2024




                                                              Yongyi Yang                                                   Cheng Long*                         Raymond Chi-Wing Wong
                                                        University of Michigan                               Nanyang Technological University                The Hong Kong University of Science
                                                                 USA                                                    Singapore                                     and Technology
                                                         yongyi@umich.edu                                          c.long@ntu.edu.sg                                    Hong Kong
                                                                                                                                                                    raywong@cse.ust.hk

                                         ABSTRACT                                                                                      1   INTRODUCTION
                                         Approximate nearest neighbor (ANN) query in high-dimensional                                  Nearest neighbor (NN) queries for vectors in high-dimensional
                                         Euclidean space is a key operator in database systems. For this                               Euclidean space are a fundamental operator in database sys-
                                         query, quantization is a popular family of methods developed for                              tems [20, 49, 58â€“60, 64, 71, 79], with a wide range of applications
                                         compressing vectors and reducing memory consumption. Recently,                                such as retrieval-augmented generation [28] and information re-
                                         a method called RaBitQ achieves the state-of-the-art performance                              trieval [40, 63]. However, due to the curse of dimensionality [34, 77],
                                         among these methods. It produces better empirical performance in                              performing exact NN queries on large-scale databases becomes im-
                                         both accuracy and efficiency when using the same compression rate                             practical because of their long response time. To balance time and
                                         and provides rigorous theoretical guarantees. However, the method                             accuracy, researchers often turn to a relaxed alternative: the ap-
                                         is only designed for compressing vectors at high compression rates                            proximate nearest neighbor (ANN) query [15, 29, 37, 44, 50].
                                         (32x) and lacks support for achieving higher accuracy by using                                   In order to respond to user queries with low latency and high
                                         more space. In this paper, we introduce a new quantization method                             accuracy, the majority of existing studies focus on the in-memory
                                         to address this limitation by extending RaBitQ. The new method                                setting of ANN which assumes that a device has sufficiently large
                                         inherits the theoretical guarantees of RaBitQ and achieves the as-                            RAM and mainly targets to optimize the search performance (the
                                         ymptotic optimality in terms of the trade-off between space and                               time-accuracy trade-off) [6, 24, 32, 43, 44]. However, in real-world
                                         error bounds as to be proven in this study. Additionally, we present                          systems, main memory is a precious resource. For example, in data-
                                         efficient implementations of the method, enabling its application to                          base systems deployed on commodity PCs [48], clouds [31, 65, 71]
                                         ANN queries to reduce both space and time consumption. Extensive                              and mobile devices [9, 13], memory consumption non-trivially af-
                                         experiments on real-world datasets confirm that our method consis-                            fects the costs of services and the experience of users. Therefore,
                                         tently outperforms the state-of-the-art baselines in both accuracy                            handling ANN queries with the minimal space usage while ensuring
                                         and efficiency when using the same amount of memory.                                          strong search performance (i.e., achieving competitive efficiency at
                                                                                                                                       90%, 95% and 99% recall) has become a crucial challenge. In partic-
                                         ACM Reference Format:
                                                                                                                                       ular, the space consumption in ANN queries is comprised of two
                                         Jianyang Gao, Yutong Gou, Yuexuan Xu, Yongyi Yang, Cheng Long* , and Ray-
                                         mond Chi-Wing Wong. 2018. Practical and Asymptotically Optimal Quanti-
                                                                                                                                       parts, one for the vectors and the other for the index. The part
                                         zation of High-Dimensional Vectors in Euclidean Space for Approximate                         for vectors usually dominates the overall consumption because
                                         Nearest Neighbor Search. In Proceedings of ACM Conference (Conferenceâ€™17).                    in most applications, they are embeddings produced by deep neu-
                                         ACM, New York, NY, USA, 16 pages. https://doi.org/XXXXXXX.XXXXXXX                             ral networks [40, 51]. Each usually has hundreds or thousands of
                                                                                                                                       dimensions and is correspondingly represented by hundreds or
                                         * Corresponding author.                                                                       thousands of floating-point numbers. Thus, many existing stud-
                                                                                                                                       ies target to reduce the memory consumption by compressing the
                                                                                                                                       vectors [1, 8, 27, 29, 37, 46].
                                         Permission to make digital or hard copies of all or part of this work for personal or
                                         classroom use is granted without fee provided that copies are not made or distributed
                                                                                                                                          To address this need, a well-known line of studies called product
                                         for profit or commercial advantage and that copies bear this notice and the full citation     quantization (PQ) has been proposed [8, 29, 37, 46, 72]. During index-
                                         on the first page. Copyrights for components of this work owned by others than ACM            ing, these methods first construct a quantization codebook. For each
                                         must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
                                         to post on servers or to redistribute to lists, requires prior specific permission and/or a   vector in the database, they find the nearest vector in the codebook
                                         fee. Request permissions from permissions@acm.org.                                            as its quantized vector, which is represented and stored as a short
                                         Conferenceâ€™17, July 2017, Washington, DC, USA                                                 quantization code. During querying, they estimate the distances
                                         Â© 2018 Association for Computing Machinery.
                                         ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
                                                                                                                                       between data and query vectors based on the quantization codes.
                                         https://doi.org/XXXXXXX.XXXXXXX                                                               However, as has been widely reported [29, 37, 46], these methods
can only produce poor recall (e.g., <80%) due to their severely lossy                             and random rotation operations on these vectors ensure that the
compression (i.e., they mostly adopt a large compression rate of                                  codebook is composed of randomly rotated unit vectors, which
â‰¥32x). To recover the recall, PQ and its variants usually adopt a                                 allows our method to inherit the unbiasedness and the error bound
re-ranking strategy [17, 72], which accesses the raw vectors and                                  of RaBitQ; and (2) the quantization code can be represented by a
computes exact distances to find the NN. However, this entails to                                 ð·-dimensional vector of ðµ-bit unsigned integers (correspondingly
store the raw vectors in main memory 1 , undermining their target                                 the size of the codebook is 2ðµÂ·ð· ) and the distance estimation can
of saving space. On the other hand, when these methods are ap-                                    be supported by their arithmetic operations without exhaustively
plied with a moderate compression rate (e.g., 8x or 4x) in order to                               decompressing the codes.
produce reasonable recall without re-ranking, they often struggle                                     Furthermore, for the new quantization codebook above, we note
to deliver competitive performance in terms of both accuracy and                                  that the task of finding the nearest vector of a data vector (when
efficiency [1, 3] (e.g., they fail to outperform the classical scalar                             computing the quantization codes) is not as easy as the original
quantization, see Section 5.2).                                                                   RaBitQ. Therefore, we propose a new efficient algorithm for this task
   A recent study proposes a new scheme of quantization called                                    during indexing. We prove that our method achieves the asymptotic
RaBitQ, which outperforms PQ and its variants both empirically                                    optimality in terms of the trade-off between space and error bounds
and theoretically [27]. Empirically, compared with PQ, when using                                 for the estimation of inner product between unit vectors. Based on
the same length of the quantization codes, the method produces                                    the experimental studies on real-world datasets (Section 5.2), we
more accurate distance estimation with less time consumption.                                     also verify that with the same number of bits, our method produces
Theoretically, unlike PQ and its variants which have no theoretical                               consistently better accuracy than the state-of-the-art method for
guarantee, RaBitQ guarantees that its distance estimation is unbi-                                compressing vectors with a moderate compression rate (e.g., 8x or
ased and has an asymptotically optimal error bound. Despite this,                                 4x).
RaBitQ has the limitation that it only supports to compress a vector                                  In addition, we apply our method to ANN queries in combination
with a large compression rate (i.e., it compresses a ð·-dimensional                                with the inverted-file index (IVF index) [37]. Beyond trivially using
floating-point vector into a ð·-dimensional binary vector, which cor-                              it for distance estimation, we find that the efficiency of the method
responds to 32x compression). In this case, without re-ranking, the                               can be further improved. In particular, in a quantization code of
method can hardly produce reasonable recall either [27]. Consider-                                our method (a vector of ðµ-bit unsigned integers), the concatenation
ing the promising performance of RaBitQ at a large compression                                    of the most significant bits of all the dimensions is exactly equal
rate, a natural question is how to extend the method to achieve                                   to the quantization code of the original RaBitQ (a vector of 0/1
moderate compression rates so as to avoid the re-ranking step which                               values). Motivated by this, we split our quantization codes and
entails storing the raw vectors. Ideally, this extension should (1)                               separately store their most significant bits and the remaining bits.
maintain accurate distance estimation with both unbiasedness and                                  During querying, we first estimate a distance by accessing only
asymptotic optimality concerning the trade-off between space and                                  the most significant bits (i.e., it produces exactly the estimated
error bounds, and (2) enable efficient distance estimation.                                       distance based of the original RaBitQ). If the estimated distance
   Nevertheless, to achieve both desiderata, the extension requires                               is sufficiently accurate to decide that a data vector cannot be the
non-trivial designs. Let ð· be the dimensionality of a vector. The                                 NN, then we drop it. Otherwise, we access the remaining bits and
original RaBitQ uses ð· bits to quantize a vector, which corresponds                               incrementally estimate a distance with higher accuracy based on the
to a large compression rate of 32x. To achieve moderate compres-                                  complete bits. Because the distance estimation based on the most
sion rates, we use (ðµ Â·ð·) bits to quantize a vector, where ðµ is a small                           significant bits can be realized with a rather efficient SIMD-based
integer. For example, when ðµ = 4 and 8, the compression rates are                                 implementation called FastScan [4] and the accuracy is sufficient
8x and 4x respectively. Correspondingly, we need to construct a                                   for pruning many data vectors, this operation helps significantly
codebook with 2ðµÂ·ð· vectors. According to the RaBitQ paper [27],                                   improve the efficiency.
(1) the unbiasedness of the estimator holds on condition that the                                     We summarize our major contributions as follows.
codebook is comprised of randomly rotated unit vectors; and (2)
the efficient computation can be achieved because the vectors in                                     (1) We propose a new quantization method by extending RaBitQ.
the codebook can be represented by binary vectors. When ðµ = 1                                            The method constructs the codebook via shifting, normaliz-
(which reduces to the case of the original RaBitQ), there exists                                         ing and randomly rotating vectors of ðµ-bit unsigned integers.
a natural construction of the codebook, which satisfies both re-                                         Based on the design, it inherits RaBitQâ€™s unbiased estima-
quirements - it constructs a codebook by randomly rotating the                                           tor for distance estimation. In addition, we prove that our
vertices of a hypercube which are nested âˆš on the unit sphere, i.e.,                                     method achieves the asymptotic optimality in terms of the
the vectors whose coordinates are Â±1/ ð· (see Section 2.2 for more                                        trade-off between space and error bounds of estimating inner
details). However, when ðµ > 1 (which is the case that we target                                          product of unit vectors.
in this paper), there is no such natural construction. To address                                    (2) We apply our method to ANN queries and introduce the effi-
the issue, we propose to construct the codebook by shifting, nor-                                        cient implementation of first estimating a distance based on
malizing and randomly rotating the vectors whose coordinates are                                         the most significant bits. When the accuracy is insufficient,
ðµ-bit unsigned integers. The rationale is that (1) the normalization                                     we access the remaining bits to incrementally estimate a
1 Another thread of studies access the raw vectors stored on disks for re-ranking [36, 73],
                                                                                                         distance with higher accuracy.
which saves memory by making a compromise in efficiency. We note that they are                       (3) We conduct extensive experiments on real-world datasets,
orthogonal to vector compression and are out of the scope of the current study.                          which show that (1) our method provides more accurate
                                                                                              2
       distance estimation and higher recall than all the baselines on       the inner product of their normalized vectors. Then, it focuses on
       all the tested datasets when using the same number of bits. At        estimating the inner product of the normalized vectors 2 .
       a compression rate of about 6.4x and 4.5x, it stably produces             During the index phase, RaBitQ constructs a set C of all possible
       over 95% and 99% recall respectively without accessing raw            bi-valued unit vectors, each consisting of values of + âˆš1 and âˆ’ âˆš1 .
                                                                                                                                         ð·         ð·
       vectors for re-ranking; (2) the empirical performance of our          Then, it randomly rotates all vectors in C by multiplying them with
       method is well aligned with the theoretical analysis.                 a random orthogonal matrix [39] (a type of Johnson-Lindenstrauss
   The remainder of the paper is organized as follows. Section 2             Transformation) to form a quantization codebook Cð‘Ÿ . The process
introduces the ANN query and preliminary techniques. Section 3               can be described with equations as follows.
                                                                                                                                      1 ð·
                                                                                                                                        
presents our method. Section 4 illustrates the application of RaBitQ                                                          1
to the in-memory ANN search. Section 5 provides extensive experi-                      Cð‘Ÿ := ð‘ƒx x âˆˆ C , ð‘¤â„Žð‘’ð‘Ÿð‘’ C := + âˆš , âˆ’ âˆš                      (3)
                                                                                                                                ð·      ð·
mental studies on real-world datasets. Section 6 discusses related
                                                                             where ð‘ƒ is a random orthogonal matrix [39]. Note that the codebook
work. Section 7 presents the conclusion and discussion.
                                                                             is solely determined by the random orthogonal matrix ð‘ƒ since the
                                                                             set of bi-valued unit vectors is pre-defined and does not rely on
2 PRELIMINARIES                                                              the data. Thus, it maintains the codebook Cð‘Ÿ conceptually only by
2.1 ANN Query                                                                sampling and storing the matrix ð‘ƒ. For each data vector o, it finds the
                                                                             nearest vector oÌ„0 in Cð‘Ÿ as its quantized vector. The quantized vector
Consider that there is a database which stores ð‘ data vectors in the
                                                                             is represented and stored as a quantization code xÌ„ð‘ âˆˆ {0, 1}ð· (a
ð·-dimensional Euclidean space. The nearest neighbor (NN) query
                                                                             ð·-bit string) - recall that each quantized vector has a corresponding
targets to find the nearest data vector from the database for a given
                                                                             bi-valued unit vector,    denoted by xÌ„0 , in C. Specifically, we have
query vector q. Due to the curse of dimensionality, the query is                              
often relaxed to the approximate nearest neighbor (ANN) query,               oÌ„0 = ð‘ƒ xÌ„0 = ð‘ƒ âˆš2 xÌ„ð‘ âˆ’ âˆš1 1ð· where 1ð· is the ð·-dimensional
                                                                                               ð·         ð·
which targets smaller time/space consumption by making a slight              vector whose coordinates are all ones.
compromise on the accuracy (e.g., it targets to reach 90%, 95% or               During the query phase, it constructs an unbiased estimator for
99% recall). In addition, the ANN query is often extended to finding         the inner product. The estimator has a theoretical error bound. We
the ð¾ nearest data vectors. For the ease of narrative, we assume             restate the estimator and its bound as follows.
that ð¾ = 1 in the algorithm description, while we note that our                                                                              âŸ¨ oÌ„ ,qâŸ©
methods can be trivially applied to the query with any arbitrary                Lemma 2.1 (Restating Theorem 3.2 in [27]). âŸ¨ oÌ„0 ,oâŸ© is an unbi-
                                                                                                                                               0
ð¾â€™s. We focus on the in-memory ANN. However, unlike most of the              ased estimator of âŸ¨o, qâŸ©. With the probability of at least 1âˆ’exp(âˆ’ð‘ 0ðœ–02 ),
existing studies on in-memory ANN which assume that the raw data             its error bound is presented as
                                                                                                               âˆšï¸„
vectors are stored in RAM [6, 43, 44], our method targets to reduce
                                                                                          âŸ¨oÌ„0, qâŸ©                1 âˆ’ âŸ¨oÌ„0, oâŸ© 2     ðœ–0
the memory consumption by compressing the raw data vectors and                                     âˆ’ âŸ¨o, qâŸ© â‰¤                    Â·âˆš                (4)
storing only the compressed vectors in main memory. That is, we                           âŸ¨oÌ„0, oâŸ©                  âŸ¨oÌ„0, oâŸ© 2
                                                                                                                                    ð· âˆ’1
target the setting where the raw vectors cannot be accessed during           where ð‘ 0 is a constant and ðœ–0 is a parameter which controls the failure
querying so as to save the main memory consumption, and this                 probability of the bound.
setting is the same as the one studied in [1].
                                                                                It is proven that using RaBitQ to quantize a ð·-dimensional vector
                                                                             to a ð·-bit string, the inner product âŸ¨oÌ„0, oâŸ© is highly concentrated
2.2    The Quantization Method RaBitQ                                        around 0.8 [27]. Thus, the above lemma indicates that for estimating
A recent paper proposes a new quantization method called Ra-                 the inner product of two ð·-dimensional
                                                                                                                  âˆš     unit vectors, it guarantees a
BitQ [27], which quantizes a ð·-dimensional real vector into a ð·-bit          probabilistic error bound of ð‘‚ (1/ ð·) with high probability, which
string. It provides an unbiased estimator of squared distances and           achieves the asymptotic optimality [2]. In terms of the computation
guarantees that the estimator has an asymptotically optimal error            of the estimator, we note that âŸ¨o, oÌ„0 âŸ© is independent of the query
bound, which always holds regardless of the data distribution.               and can be pre-computed before querying. The computation of
    Specifically, given a raw data vector oð‘Ÿ and a raw query vector          âŸ¨q, oÌ„0 âŸ© can be conducted as follows.
qð‘Ÿ , it first normalizes the vectors based on a vector c (e.g., the                                          
                                                                                                                2         1
                                                                                                                                
                                                                q âˆ’c
centroid of a set of data vectors). Let o := âˆ¥ooð‘Ÿ âˆ’c  and q := âˆ¥qð‘Ÿ âˆ’câˆ¥                        âŸ¨q, oÌ„0 âŸ© = q, ð‘ƒ âˆš xÌ„ð‘ âˆ’ âˆš 1ð·                       (5)
                                                ð‘Ÿ âˆ’câˆ¥            ð‘Ÿ                                              ð·         ð·
be the normalized data and query vectors. The (squared) Euclidean                                                            ð·
distance between oð‘Ÿ and qð‘Ÿ can be expressed as follows.                                                     2            1 âˆ‘ï¸ â€²
                                                                                                        = âˆš qâ€², xÌ„ð‘ âˆ’ âˆš         q [ð‘–]             (6)
                                                                                                            ð·             ð· ð‘–=1
       âˆ¥oð‘Ÿ âˆ’ qð‘Ÿ âˆ¥ 2 = âˆ¥(oð‘Ÿ âˆ’ c) âˆ’ (qð‘Ÿ âˆ’ c)âˆ¥ 2                      (1)       Here, qâ€² denotes ð‘ƒ âˆ’1 q and qâ€² [ð‘–] denotes the ð‘–-th dimension of the
                2            2
      =âˆ¥oð‘Ÿ âˆ’ câˆ¥ + âˆ¥qð‘Ÿ âˆ’ câˆ¥ âˆ’ 2 Â· âˆ¥oð‘Ÿ âˆ’ câˆ¥ Â· âˆ¥qð‘Ÿ âˆ’ câˆ¥ Â· âŸ¨q, oâŸ©      (2)       vector qâ€² ; (5) plugs in the definition of oÌ„ and (6) applies ð‘ƒ âˆ’1 on
                                                                                                                          Ãð· â€²
                                                                             both sides of the inner product. Note that ð‘–=1    q [ð‘–] depends only
Note that the distance âˆ¥or âˆ’ câˆ¥ can be pre-computed in the index             on the query vector. Thus, its computation can be conducted once
phase and âˆ¥qð‘Ÿ âˆ’ câˆ¥ can be computed when a query comes and can                and shared by many data vectors. For the computation of âŸ¨qâ€², xÌ„ð‘ âŸ©,
be shared by many data vectors. Therefore, the computation of                2Without further specification, in this paper, by data and query vectors, we refer to
the distances between the raw vectors can be reduced to that of              their normalized vectors.
                                                                         3
[27] introduces two versions of implementation. One is based on
a SIMD-based implementation called FastScan [4], which can effi-
ciently compute the estimated distances for data vectors batch by
batch. The other is based on bitwise operations, which supports
to efficiently estimate distances for individual vectors. We refer
readers to the original papers of RaBitQ [27] and FastScan [3â€“5]
for more technical details and theoretical analysis. With all the pro-
posed techniques, RaBitQ supports to unbiasedly estimate the inner
product (and further unbiasedly estimate the squared distances)
with both promising accuracy and efficiency.                                                Figure 1: This figure illustrates the quantization codebook
                                                                                            of our method when ðµ = 2 in the 2-dimensional space. The
3 EXTENDING RABITQ                                                                          empty blue points in the left panel represent the set G, i.e., a
3.1 Motivations and Overview                                                                set of vectors on a uniform grid. The solid red points in the
                                                                                            right panel represent the normalized vectors in G. Applying
For ANN query, to avoid storing raw vectors in RAM and reach
                                                                                            a random rotation on the red points yields the codebook Gð‘Ÿ .
promising recall at the same time, it is necessary to compress
the vectors with a moderate compression rate. Although RaBitQ                               the vectors can be hosted in a ð·-dimensional space. In contrast, it
achieves an asymptotically optimal error bound when quantizing                              pads these vectors to (ðµ Â· ð·) dimensions and applies RaBitQ in the
ð·-dimensional vectors into ð·-bit codes (which corresponds to a                              (ðµ Â· ð·)-dimensional space. The codebook has the size of 2ðµÂ·ð· and
rather high compression rate), it is still unclear how it can use                           thus, it produces the quantization codes of (ðµ Â· ð·) bits. In intuition,
more bits in pursuit of higher accuracy. In [27], it presents a simple                      given the same number of bits, a quantization algorithm would
way to use more bits by padding the vectors. Specifically, it pads                          produce larger error when the vectors have higher dimensionality
ð·-dimensional vectors to (ðµ Â· ð·) dimensions with zeros, and thus, it                        itself. Thus, it is likely that the sub-optimal accuracy is caused by
can use (ðµÂ·ð·) bits by directly applying RaBitQ. Based âˆš on Lemma
                                                           âˆš       2.1,                     the padding operation.
in this case, it guarantees an error bound of ð‘‚ (1/( ðµ Â· ð·)) with                              Instead of padding the vectors and constructing a codebook
high probability. However, this result shows that the error decays                          with 2ðµÂ·ð· vectors in the (ðµ Â· ð·)-dimensional space, we consider
very slowly with respect to the number of bits used, indicating that                        constructing a codebook with 2ðµÂ·ð· vectors in the ð·-dimensional
this simple extension is not very effective for the scenarios which                         space. Nevertheless, to inherit the unbiasedness and error bound of
require a moderate compression rate.                                                        the estimator in RaBitQ and support efficient computation at the
   To be more formal, we note that, with this simple extension,                             same time, the construction of the codebook is highly restrictive.
to guarantee an error  bound ðœ– with the failure probability of at                         First, the unbiasedness and the error bound (Lemma 2.1) hold on
most ð›¿, it requires Î˜ ðœ–12 log ð›¿1 bits 3 . However, as has been proven                       condition that the codebook is constructed by randomly rotating
by a theoretical study [2], to guarantee                                                    a set of unit vectors. Second, for efficient computation, it should
                                            an error
                                                       bound ðœ–,
                                                                it is                     better support to directly compute the inner product based on the
sufficient (and also necessary) to use Î˜ ð· log ð·1 Â· ðœ–12 log ð›¿1 bits                         quantization codes without an exhaustive decompression step. We
when the target accuracy is high 4 , i.e., when ðœ–12 log ð›¿1 > ð·. Note                        propose the following codebook Gð‘Ÿ to meet both requirements.
that the required number of bits in this result of the theoretical                                              ðµ                                        ð·
                                                                                                                   2 âˆ’1
study is logarithmic to ðœ– âˆ’2 while the one in the former result of the                                  G := âˆ’            + ð‘¢ ð‘¢ = 0, 1, 2, 3, ..., 2ðµ âˆ’ 1       (7)
simple way of padding vectors is linear to ðœ– âˆ’2 . For instance, when                                                  2
 1 log 1 = ðµ Â· ð·, the gap would be that between Î˜ (ð· log ðµ) and
                                                                                                                              
                                                                                                                    y
ðœ–2      ð›¿                                                                                              Gð‘Ÿ := ð‘ƒ           yâˆˆG                                    (8)
Î˜(ðµ Â· ð·). This implies that when targeting higher accuracy with                                                    âˆ¥yâˆ¥
more bits used, there is still substantial room to improve from the                         Specifically, as is illustrated in Figure 1, we first consider the set
simple extension of RaBitQ.                                                                 of vectors on the uniform grid G. Then we adopt their normalized
   Motivated by the discussions above, in this paper, we propose a                                          y
                                                                                            vectors (i.e., âˆ¥yâˆ¥ in Equation (8)) and randomly rotate them by
new quantization method, which extends RaBitQ to support moder-                             multiplying them with a random orthogonal matrix ð‘ƒ) to form the
ate compression rates and achieves asymptotically optimal estima-                           codebook Gð‘Ÿ . The rationale is that (1) Gð‘Ÿ is composed of randomly
tion and efficient computation at the same time. Next, we present                           rotated unit vectors. Thus, it inherits the estimator in Lemma 2.1
the details of our method, including (1) how it quantizes data vectors                      along with its unbiasedness and error bound 5 ; and (2) the vectors
(Section 3.2), (2) how it computes distance estimators (Section 3.3),                       are generated by shifting, normalizing and rotating the vectors
and (3) its summary and theoretical results (Section 3.4).                                  that consist of ðµ-bit unsigned integers (Equation (7) and (8)). This
                                                                                            enables efficient computation of inner product (see Section 3.3 for
3.2     Quantizing Data Vectors                                                             details).
3.2.1 Constructing a Codebook. We notice that the simple exten-                                In practice, the construction of the codebook is very simple. We
sion in [27] provides sub-optimal accuracy because it ignores that                          only need to sample a random transformation matrix ð‘ƒ as RaBitQ
3 This can be derived from the result in Lemma 2.1 by plugging in the parameters of ðœ–       5 For the new codebook, Lemma 2.1 exactly holds. However, it is worth noting that
and ð›¿ .                                                                                     the inner product between the data vector and the quantized data vector in the new
4 The result is restated directly from Theorem 4.1 in [2].                                  codebook increases with respect to ðµ . Thus, the error of the estimator decreases.
                                                                                        4
does [27], and then the codebook Gð‘Ÿ is determined. To store the                    âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥ 7 . ð‘£ð‘šð‘Žð‘¥ maintains the maximum value of âŸ¨yð‘ð‘¢ð‘Ÿ /âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥, oâ€² âŸ©
codebook Gð‘Ÿ , we only need to store the sampled ð‘ƒ, i.e., we maintain               based on the vectors that have been enumerated so far. ð‘¡ð‘šð‘Žð‘¥ main-
the codebook Gð‘Ÿ , which contains 2ðµÂ·ð· vectors, conceptually. In                    tains the re-scaling factor which produces the maximum value ð‘£ð‘šð‘Žð‘¥ .
particular, it is worth noting that when setting ðµ = 1, the codebook               The enumeration starts from ð‘¡ = 0 (line 1-2). Then iteratively, we
is exactly the same as that of the original RaBitQ.                                enumerate the next smallest critical value (line 3-4). Based on the
                                                                                   new re-scaling factor ð‘¡, the vector yð‘ð‘¢ð‘Ÿ will change in only one di-
3.2.2 Computing the Quantization Codes of Data Vectors. Next we
                                                                                   mension. Thus, we can update yð‘ð‘¢ð‘Ÿ , âŸ¨yð‘ð‘¢ð‘Ÿ , oâ€² âŸ© and âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥ in ð‘‚ (1)
find for each data vector its nearest vector oÌ„ in the codebook Gð‘Ÿ
                                                                                   time (line 5). During the enumeration, we record the re-scaling
as its quantized vector. Formally, for a vector o, we target to find
                                                                                   factor which produces the maximum âŸ¨yð‘ð‘¢ð‘Ÿ /âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥, oâ€² âŸ© (line 6-7).
oÌ„ âˆˆ Gð‘Ÿ such that âˆ¥ oÌ„ âˆ’ oâˆ¥ 2 is minimized. Let yÌ„ be the corresponding
                                                                                   The enumeration terminates when all the critical values have been
vector of oÌ„ in G, i.e., oÌ„ = ð‘ƒ yÌ„/âˆ¥ yÌ„âˆ¥. Following [27], we simplify the
                                                                                   enumerated (line 3). Finally, based on the re-scaling factor ð‘¡ð‘šð‘Žð‘¥ , we
problem as follows.
                                                                                   find the yÌ„ via re-scaling and rounding (line 8). We represent yÌ„ and
                                    2                              
                          y                                     y                  store it as a vector of unsigned integers yÌ„ð‘¢ . Specifically, we have
       yÌ„ = arg min ð‘ƒ         âˆ’ o = arg min 2 âˆ’ 2 ð‘ƒ               ,o     (9)
              yâˆˆ G       âˆ¥yâˆ¥               yâˆˆ G               âˆ¥yâˆ¥                  yÌ„ð‘¢ = yÌ„ + (2ðµ âˆ’ 1)/2 Â· 1ð· where 1ð· is the vector whose coordinates
                                                                               are all ones (line 9).
                           y                       y
          = arg max ð‘ƒ         , o = arg max           , ð‘ƒ âˆ’1 o          (10)          The overall time complexity of the algorithm is ð‘‚ (2ðµ Â· ð· log ð·)
              yâˆˆ G       âˆ¥yâˆ¥             yâˆˆ G     âˆ¥yâˆ¥
                                                                                   because in total we enumerate (2ðµâˆ’1 Â· ð·) critical values and a min-
where Equation (9) is derived from the definition of yÌ„; Equation                  heap is needed to find the next smallest critical value during the
(10) applies an orthonormal matrix ð‘ƒ âˆ’1 to both sides of the inner                 process (its maintenance takes ð‘‚ (log ð·) time). We note that this
product. For conciseness, let us denote oâ€² := ð‘ƒ âˆ’1 o. Now the ques-                time complexity is good enough for practical usage as our algorithm
tion reduces to one of finding yÌ„ âˆˆ G such that Equation (10) is                   is designed for vector compression, i.e., ðµ is small. According to
maximized.                                                                         our experimental studies in Section 5.2.2, ðµ = 7 suffices to stably
   A natural idea is to enumerate the vectors in G, compute the                    produce > 99% recall and ðµ = 5 suffices to stably produce > 95%
value of Equation (10) for each vector and find the vector yÌ„. How-                recall. Under these settings, the quantization of a million-scale
ever, enumerating all the vectors in G is not feasible as G contains               dataset of 3,072 dimensions can finish in a few minutes.
2ðµÂ·ð· vectors. We observe that the enumeration can be significantly
pruned based on the following lemma. The proof is left in Appen-                    Algorithm 1: Quantize
dix A due to page limit.
                                        D      E                                       Input : A ð·-dimensional vector oâ€² ; the number of bits per
                                           y
   Lemma 3.1. Let yÌ„ = arg maxyâˆˆ G âˆ¥yâˆ¥ , oâ€² . Then âˆƒð‘¡ > 0 such                                  dimension ðµ.
that âˆ€y âˆˆ G, âˆ¥ð‘¡ Â· oâ€² âˆ’ yÌ„âˆ¥ â‰¤ âˆ¥ð‘¡ Â· oâ€² âˆ’ yâˆ¥.                                             Output : The quantization code yÌ„ð‘¢ .
                                                                                   1 ð‘¡ â† 0, ð‘£ð‘šð‘Žð‘¥ â† 0, ð‘¡ð‘šð‘Žð‘¥ â† 0
    In intuition, this lemma indicates that for a vector oâ€² , if a vector
yÌ„ has the largest cosine similarity from oâ€² among the set G, then                 2 Initialize yð‘ð‘¢ð‘Ÿ , âŸ¨yð‘ð‘¢ð‘Ÿ , oâ€² âŸ© and âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥ with ð‘¡ = 0
                                                                                   3 while some critical values have not been enumerated do
there must be a re-scaling factor ð‘¡ such that yÌ„ also has the smallest
Euclidean distance from the re-scaled vector ð‘¡ Â· oâ€² among the set                  4     Update ð‘¡ with the next smallest critical value
G. Note that G is a set of vectors on uniform grids, the nearest                   5     Update yð‘ð‘¢ð‘Ÿ , âŸ¨yð‘ð‘¢ð‘Ÿ , oâ€² âŸ© and âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥ with the new ð‘¡
vector of ð‘¡ Â· oâ€² in G can be easily computed by rounding. This is                  6     if âŸ¨yð‘ð‘¢ð‘Ÿ , oâ€² âŸ© /âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥ > ð‘£ð‘šð‘Žð‘¥ then
to say, if we â€œenumerate every re-scaling factor ð‘¡â€ and collect all                7          ð‘£ð‘šð‘Žð‘¥ â† âŸ¨yð‘ð‘¢ð‘Ÿ , oâ€² âŸ© /âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥, ð‘¡ð‘šð‘Žð‘¥ â† ð‘¡
the vectors produced by rounding ð‘¡ Â· oâ€² , then the target vector yÌ„
                                                                                   8 Compute yÌ„ via re-scaling and rounding oâ€² with ð‘¡ð‘šð‘Žð‘¥
must be included in the vectors that are collected in the process.                                                 ðµ
                                                                                   9 return yÌ„ð‘¢ where yÌ„ð‘¢ = yÌ„ + (2 âˆ’ 1)/2 Â· 1
In practice, we do not need to enumerate every real value ð‘¡ > 0
because when two re-scaling factors are extremely close to each
other, they would produce exactly the same vector after rounding.
Thus, we only need to enumerate the critical values which change
the results of rounding. Specifically, for a given ð‘¡, assume that                  3.3     Computing the Estimator
ð‘¡ Â· oâ€² [ð‘–] is currently rounded to a number ð‘¥. Then the next critical              Recall that we target to estimate the inner product âŸ¨o, qâŸ© to further
value of ð‘¡ that rounds ð‘¡ Â· oâ€² [ð‘–] to (ð‘¥ + 1) is (ð‘¥ + 0.5)/oâ€² [ð‘–]. As there         estimate the squared distances (Section 2.2). We adopt the estimator
are ð· dimensions and each dimension has 2ðµâˆ’1 different values                      of RaBitQ to inherit its unbiasedness and error bound 8 (see empiri-
after rounding 6 , we will in total enumerate up to (ð· Â· 2ðµâˆ’1 ) critical           cal verification in Section 5.2.1), i.e., we use âŸ¨oÌ„, qâŸ© /âŸ¨oÌ„, oâŸ© to estimate
values and correspondingly, up to (ð· Â· 2ðµâˆ’1 ) vectors in G.                        âŸ¨o, qâŸ©. The denominator âŸ¨oÌ„, oâŸ© is only related to the data vector and
    To efficiently implement this idea, we enumerate the critical                  its quantized vector, so it can be pre-computed in the index phase.
values in an ascending order. The algorithm is presented in Al-
                                                                                                                                       â€²
                                                                                          ð‘ð‘¢ð‘Ÿ , we use two variables to store âŸ¨yð‘ð‘¢ð‘Ÿ , o âŸ© and âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥ . Whenever yð‘ð‘¢ð‘Ÿ is
                                                                                   7 For y
gorithm 1. In this process, we dynamically maintain the vector
                                                                                   updated, we can update these variables to obtain the new values of âŸ¨yð‘ð‘¢ð‘Ÿ , oâ€² âŸ© and
produced by rounding with yð‘ð‘¢ð‘Ÿ (the vector that is currently be-                   âˆ¥yð‘ð‘¢ð‘Ÿ âˆ¥ efficiently.
ing enumerated). We use two variables to maintain âŸ¨yð‘ð‘¢ð‘Ÿ , oâ€² âŸ© and                 8 This conclusion can be directly yielded from the proof in the original RaBitQ pa-
                                                                                   per [27], i.e., Lemma 2.1 holds if (1) the codebook is a set of randomly rotated unit
6 It is 2ðµâˆ’1 instead of 2ðµ because yÌ„ is in the same orthant of oâ€² .               vectors; and (2) oÌ„ is the nearest vector of o in the codebook.
                                                                               5
                                                                 yÌ„
Thus, we only need to compute âŸ¨oÌ„, qâŸ©. Recall that oÌ„ = ð‘ƒ âˆ¥ yÌ„âˆ¥ . The               Remark (Empirical Formula). Let ðœ– be the absolute error of
following equations illustrate how it can be computed.                           estimating inner product
                                                                                                     âˆš    of unit vectors. With >99.9% probability, we
                                                                             have ðœ– < 2 âˆ’ðµ Â· ð‘ðœ– / ð· where ð‘ðœ– = 5.75.
                        yÌ„             yÌ„                1
        âŸ¨oÌ„, qâŸ© = ð‘ƒ         ,q =           , ð‘ƒ âˆ’1 q =         yÌ„, qâ€² (11)
                      âˆ¥ yÌ„âˆ¥          âˆ¥ yÌ„âˆ¥             âˆ¥ yÌ„âˆ¥                        We note that the constant in this empirical formula is measured
                                                   ð·
                                                            !                    with experimental studies, i.e., we use our algorithm to estimate
                    1                   2ðµ âˆ’ 1 âˆ‘ï¸ â€²
                =           yÌ„ð‘¢ , qâ€² âˆ’               q [ð‘–]           (12)        inner product and collect the statistics of errors (see Section 5.2.6).
                  âˆ¥ yÌ„âˆ¥                     2 ð‘–=1
                                                                                 Comparison with the Algorithmic Proof in [2]. In the the-
Here qâ€² denotes ð‘ƒ âˆ’1 q; Equation (11) applies an orthonormal matrix              oretical study [2], there is an algorithmic proof which achieves
ð‘ƒ âˆ’1 to both sides of the inner product; and Equation (12) expresses             the asymptotic optimality. However, we note that the algorithmic
yÌ„ with its quantization code yÌ„ð‘¢ , i.e., yÌ„ = yÌ„ð‘¢ âˆ’ (2ðµ âˆ’ 1)/2 Â· 1ð· .           proof is less practically applicable. Specifically, this proof relies on
   Note that âˆ¥ yÌ„âˆ¥ is only related to the quantized vectors and can              the operation which represents different integers with different
                                            Ãð· â€²                                 number of bits. Based on this operation, [2] proves that the total
be pre-computed in the index phase. ð‘–=1           q [ð‘–] is only related to
the query vector. It can be computed once and its time costs can                 number of bits needed is asymptotically optimal. This algorithm
be shared by many data vectors. Thus, the remaining task is the                  is unfriendly to real-world systems because it is unclear how a
computation of âŸ¨yÌ„ð‘¢ , qâ€² âŸ©, i.e., the inner product between a vector of          sequence of integers represented by different number of bits can
unsigned integers and a vector of floating-point numbers. When ðµ =               be stored in alignment with each other. In contrast, our method
1 (the case of the original RaBitQ), RaBitQâ€™s implementation can be              achieves both the asymptotic optimality and the practicality. To the
directly applied [27]. When ðµ equals to 4 or 8, the implementations              best of our knowledge, our study is also the first which achieves
in existing systems (for computing the inner product between a                   both desiderata at the same time. We would like to emphasize that
vector of 4-bit or 8-bit unsigned integers and a vector of floating-             our work does not target the improvement in terms of theory. In-
point numbers) can be directly applied [1, 17]. Other settings of ðµâ€™s            stead, we propose a practically applicable algorithm and prove that
can be implemented by splitting a vector of ðµ-bit unsigned integers              it is asymptotically optimal.
into several parts, where each part has the size of the power of 2
(e.g., a vector of 9-bit unsigned integers can be split into a binary            4  APPLYING THE EXTENDED RABITQ TO
vector and a vector of 8-bit unsigned integers). We will discuss the                IN-MEMORY ANN
details of the idea later in Section 4.2.
                                                                                 4.1 Using the Extended RaBitQ with IVF
3.4    Summary and Theoretical Analysis                                          Next we apply the extended RaBitQ method to the in-memory ANN.
                                                                                 Recall that as is discussed in Section 1, the present study targets to
We summarize the workflow of the extended RaBitQ as follows. In                  compress vectors with a moderate compression rate such that it
the index phase, the algorithm constructs a quantization codebook                can avoid storing raw vectors in RAM while still producing good
by sampling a random orthogonal matrix ð‘ƒ. It then applies ð‘ƒ âˆ’1 to                overall recall (e.g., >90%, >95% or >99%). Scalar quantization (SQ)
the data vectors, normalizes them to obtain oâ€² and computes their                and its variants are the state-of-the-art and also the most popular
quantization codes via Algorithm 1. In the query phase, when a                   methods in real-world systems for this need [1, 17, 52, 71]. We note
query comes, it first applies ð‘ƒ âˆ’1 to the query vector and normalizes            that these methods are usually used together with IVF [17, 52, 71]
it to obtain qâ€² . It can then unbiasedly estimate squared distances              or graph-based indices [1] for in-memory ANN query. In particular,
based on Equation (12) and Equation (2).                                         IVF is a popular method that has been widely deployed in real-
    Recall that as has been proved in [2] and discussed in Section 3.1,          world systems for vector search due to its simplicity and effective-
to achieve an error bound ðœ– where ðœ–12 log ð›¿1 > ð·, the minimum                    ness [17, 52, 71]. It has tiny index size and can be easily combined
                                                    
required number of bits is Î˜ ð· log ð·1 Â· ðœ–12 log ð›¿1 . The follow-                 with various quantization methods due to its sequential memory
ing theorem presents that our method achieves this asymptotic                    access pattern in querying. For graph-based indices, we note that
optimality. The proof is left in Appendix B due to page limit.                   due to their random memory access pattern of querying, combin-
                                                                                 ing them with SQ entails highly non-trivial efforts in engineering
   Theorem 3.2. For ðœ– > 0 where ðœ–12 log ð›¿1 > ð·, to ensure that the               optimization to make the combined method work competitively [1].
error of the estimator is bounded by ðœ– with
                                            the probability
                                                         of at least           Thus, in this paper, we focus on using the extended RaBitQ method
1 âˆ’ ð›¿, the algorithm requires ðµ = Î˜ log ð·1 Â· ðœ–12 log ð›¿1      .                   together with the IVF index and leave its combination with the
                                                                                 graph-based indices as future work.
    It is worth noting that the number of bits needed for each di-                  Specifically, the IVF method partitions the set of data vectors
mension ðµ is logarithmic wrt ðœ– âˆ’2 and is negatively related to the               into many clusters (e.g., via KMeans) in the index phase. Then
dimensionality ð·. Besides the asymptotic analysis, to provide a                  when a query comes, it finds a few nearest centroids of the clusters
more quantitative reference for practitioners, we would also like                and considers the vectors in these clusters as candidates of ANN.
to present an empirical formula about the error. Note that when                  When IVF is used in combination with SQ, it computes an estimated
ðµ = 1âˆš(the original setting of RaBitQ [27]), the error is bounded by             distance for every candidate based on its quantization code and
ð‘‚ (1/ ð·) with high probability. When a larger ðµ is used, the error               then returns the vector with the smallest estimated distance as the
is supposed to decay exponentially. Thus, we present the empirical               NN. Note that for reducing the memory consumption, when IVF is
formula in the following form.                                                   used with SQ, it does not store the raw vectors in RAM and thus,
                                                                             6
there is no re-ranking based on the raw vectors. When using the                   distance based on the full bits as follows.
                                                                                                           D                         E
same number of bits for quantization, our method can produce
                                                                                                 yÌ„ð‘¢ , qâ€² = 2ðµâˆ’1 Â· yÌ„0 + yÌ„ð‘™ð‘Žð‘ ð‘¡ , qâ€²                           (13)
better accuracy than SQ and its variants (see experimental results
                                                                                                                  ðµâˆ’1           â€²                 â€²
in Section 5.2.1). In addition, its computation can be conducted                                             =2     Â· yÌ„0, q + yÌ„ð‘™ð‘Žð‘ ð‘¡ , q           (14)
with exactly the same implementations of SQ (Section 3.3). Thus, it                                        â€²
                                                                                  In particular, as âŸ¨yÌ„0, q âŸ© has been computed and can be reused, we
is expected that simply replacing SQ with our method suffices to
                                                                                  only need to access yÌ„ð‘™ð‘Žð‘ ð‘¡ and compute âŸ¨yÌ„ð‘™ð‘Žð‘ ð‘¡ , qâ€² âŸ©. When ðµ = 5 and
produce better time-accuracy trade-off (i.e., it improves the accuracy
                                                                                  ðµ = 9, yÌ„ð‘™ð‘Žð‘ ð‘¡ corresponds to vectors of 4-bit and 8-bit unsigned inte-
while maintaining the efficiency).
                                                                                  gers respectively and the computation of âŸ¨yÌ„ð‘™ð‘Žð‘ ð‘¡ , qâ€² âŸ© can be realized
                                                                                  with existing implementations [1, 17, 82]. Moreover, we provide
                                                                                  the implementations (including designs in both compact storage
                                                                                  and efficient computation of inner product) for more settings of ðµâ€™s
                                                                                  (ðµ = 3, 4, 7, 8) as they also provide valuable trade-off among space,
                                                                                  time and accuracy. Due to the limit of space, we leave the details in
                                                                                  our code repository.
                        ...                                    ...
                                                                                  5 EXPERIMENTS
   Figure 2: Decomposition of the Quantization Code yÌ„ð‘¢ .
                                                                                  5.1 Experimental Setup
4.2    Conducting Distance Comparison with the                                    Experimental Platform. All experiments are run on a ma-
       Extended RaBitQ                                                            chine with two Intel Xeon Gold 6418H@4.0GHz CPUs (with Sap-
                                                                                  phire Rapids architecture, 48 cores/96 threads) and 1TB RAM.
Beyond trivially replacing SQ with our extended RaBitQ, we note                   The C++ source codes are compiled by GCC 11.4.0 with -Ofast
that the efficiency of the method can be further improved. In par-                -march=native under Ubuntu 22.04 LTS. For all methods, by follow-
ticular, a recent study finds that to reliably find the NN from a set             ing most of the existing studies and benchmarks [6, 24], the search
of candidates, it is not necessary to compute an exact distance for               performance is evaluated in a single thread and the indexing time
every candidate [26]. If an estimated distance can confirm that a                 is measured using multiple threads. The source code is available at
candidate is unlikely to be the NN (e.g., a lower bound of the esti-              https://github.com/VectorDB-NTU/Extended-RaBitQ.
mated distance is greater than the distance of the NN that has been
                                                                                  Datasets. We evaluate the performance of different algorithms
searched so far), then the candidate can be discarded. Otherwise,
                                                                                  with six public real-world datasets with varying dimensionality and
it can incrementally compute a more accurate estimated distance
                                                                                  data types, whose details are presented in Table 1. These datasets
until (1) it can confirm that a candidate is unlikely to be the NN; or
                                                                                  encompass both (1) widely adopted benchmarks for evaluating ANN
(2) the exact distance is computed [26].
                                                                                  algorithms [6, 27, 43] (such as Word2Vec, MSong and GIST 9 ) and
    Based on the idea above, in our case, we note that it is not
                                                                                  (2) embeddings generated by advanced models (including OpenAI-
always necessary to estimate a highly accurate distance with all
                                                                                  1536 10 , OpenAI-3072 11 , Youtube 12 , and MSMARCO 13 ). It is worth
bits of a quantization code. Instead, we first use a subset of bits of
                                                                                  highlighting that OpenAI-1536 and OpenAI-3072 are produced
the quantization code to efficiently estimate a distance with lower
                                                                                  by the most powerful embedding model text-embedding-3-large
accuracy. If this estimated distance can confirm that a candidate
                                                                                  of OpenAI published in early 2024 [51]. They are generated for
is unlikely to be the NN, then we discard it. Otherwise, we access
                                                                                  evaluating ANN algorithms by Qdrant [60]. As for the set of query
the remaining bits to compute a high-accuracy estimated distance
                                                                                  vectors, for the datasets which provide the query set themselves,
based on the full bits of the quantization code.
                                                                                  we adopt their query set for testing. For others, we exclude 1,000
    Specifically, in the quantization code yÌ„ð‘¢ , every dimension cor-
                                                                                  vectors from each dataset and use them as the query vectors.
responds to an unsigned integer of ðµ bits. We observe that the
concatenation of the most significant bits of all the dimensions of               Algorithms. In the experiments regarding the trade-off be-
yÌ„ð‘¢ (see Figure 2) exactly equals to the quantization code xÌ„ð‘ of the             tween the space and the accuracy of distance estimation (Sec-
original RaBitQ. This is because both of them are decided by the                  tion 5.2.1), we evaluate the performance of the following meth-
orthant of the vector oâ€² . This motivates us to split a quantization              ods. (1) RaBitQ (ext) is the extended RaBitQ method proposed
code yÌ„ð‘¢ into two parts, the code of the most significant bits yÌ„0 and            in the current study. (2) RaBitQ (pad) is the simple extension of
the code of the remaining bits yÌ„ð‘™ð‘Žð‘ ð‘¡ , where yÌ„ð‘¢ = 2ðµâˆ’1 Â· yÌ„0 + yÌ„ð‘™ð‘Žð‘ ð‘¡ and       RaBitQ mentioned in the original RaBitQ paper which uses more
yÌ„0 = xÌ„ð‘ . In the query phase, for a set of candidates, we first estimate        bits by padding the vectors with zeros [27]. (3) SQ is the classic
their distances based on their yÌ„0 â€™s, i.e., we compute âŸ¨yÌ„0, qâ€² âŸ© with           uniform scalar quantization method. It has been widely deployed
FastScan [4] as the original RaBitQ does, and further compute the                 in real-world systems [11, 52, 71, 79]. Specifically, SQ first collects
estimated distances based on Lemma 2.1 and Equation (2). Note that                9 https://www.cse.cuhk.edu.hk/systems/hash/gqr/datasets.html.
this estimated distance is exactly the estimated distance produced                10 https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-

by the original RaBitQ and it has a theoretical error bound. Then                 3-large-1536-1M
                                                                                  11 https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-
we use the bound to decide whether the candidate is unlikely to be
                                                                                  3-large-3072-1M
the NN. If so, we drop the candidate. Otherwise, we access the code               12 https://research.google.com/youtube8m/download.html

of the remaining bits yÌ„ð‘™ð‘Žð‘ ð‘¡ to incrementally compute an estimated                13 https://huggingface.co/datasets/Cohere/msmarco-v2.1-embed-english-v3

                                                                              7
                         Table 1: Dataset Statistics                                           error and (2) the maximum relative error on the estimated squared
    Dataset                 Size       ð·     Query Size                Data Type               distances. We measure the space with the number of bits per dimen-
    MSong                 992,272     420        200                    Audio                  sion. Specifically, it sums up all the space consumption of a vector
    Youtube               999,000    1,024      1,000                   Video                  and divides it by the dimensionality. Note that for our method, it
  OpenAI-1536             999,000    1,536      1,000                    Text                  also covers the space for storing two floating-point numbers for
  OpenAI-3072             999,000    3,072      1,000                    Text                  every vector, i.e., âˆ¥oð‘Ÿ âˆ’ câˆ¥ and 1/(âˆ¥yð‘¢ âˆ¥ Â· âŸ¨o, oÌ„âŸ©). In the experiments
   Word2Vec              1,000,000    300       1,000                    Text                  regarding the ANN query (Section 5.2.2), we adopt recall and aver-
     GIST                1,000,000    960       1,000                   Image                  age distance ratio for measuring the accuracy of ANN search. Recall
  MSMARCO               113,520,750 1,024       1,677                    Text                  is the percentage of successfully retrieved true nearest neighbors.
                                                                                               Average distance ratio is the average of the distance ratios of the
                                                                                               retrieved nearest neighbors over the true nearest neighbors. These
the minimum value ð‘£ð‘™ and maximum value ð‘£ð‘Ÿ among all the coor-                                  metrics are widely adopted to measure the accuracy of ANN algo-
dinates of all the vectors. Then, the method uniformly splits the                              rithms [6, 25, 33, 43, 56, 66]. We adopt query per second (QPS), i.e.,
range of values [ð‘£ð‘™ , ð‘£ð‘Ÿ ] into 2ðµ âˆ’ 1 segments. Each floating point                           the number of queries a method can handle in a second, to measure
number is then rounded to its nearest boundary of the segments                                 the efficiency. It is widely adopted to measure the efficiency of
and is represented and stored as a ðµ-bit unsigned integer. (4) LVQ                             ANN algorithms [6, 43, 74]. Following [6, 43, 74], the query time is
is the latest variant of SQ [1]. Different from SQ which collects                              evaluated in a single thread and the search is conducted for each
the smallest and largest values ð‘£ð‘™ and ð‘£ð‘Ÿ among all the vectors and                            query individually (instead of queries in a batch). All the metrics
performs quantization based on this global range of values [ð‘£ð‘™ , ð‘£ð‘Ÿ ],                         are measured on every single query and averaged over the whole
LVQ collects the ð‘£ð‘™ and ð‘£ð‘Ÿ for every individual vector and performs                            query set. We also report the time costs of different methods in the
quantization of a vector based on its specific range of values [ð‘£ð‘™ , ð‘£ð‘Ÿ ].                     index phase.
(5) PQ and (6) OPQ are popular quantization methods which are
                                                                                               Parameter Settings in ANN Query. In the IVF index, we set the
usually used with a large compression rate. They are widely de-                                number of clusters to be 4,096 for million-scale datasets by following
ployed in real-world systems [52, 71, 79]. Note that these methods                             the suggestions of Faiss [21]. For the MSMARCO dataset, we use
have two settings ð‘˜ = 4 or ð‘˜ = 8 (ð‘˜ is the number of bits allo-                                262,144 clusters. For our method, we implement our algorithm
cated for quantizing each sub-vector in these methods; see details                             with ðµ = 3, 4, 5, 7, 8, 9 and conduct ANN query with the strategy
in their original papers [29, 37]). Since ð‘˜ = 8 produces consistently                          presented in Section 4. For the alignment of data, we pad the vectors
better space-accuracy trade-off than ð‘˜ = 4, we report the results                              with zeros such that their dimensionality is a multiple of 64 (i.e., we
of PQ and OPQ with ð‘˜ = 8. It is worth noting that when using                                   pad the dimensionality of MSong to 448 and that of Word2Vec to
the same number of bits per dimension, SQ, RaBitQ (ext) and LVQ                                320). For LVQ, we apply the settings ðµ = 4 and ðµ = 8 in its original
have almost the same efficiency in computing inner product and                                 paper [1].
Euclidean distances. However, as has been reported [1, 27], PQ and
OPQ have significantly worse efficiency since their computation
relies on frequently looking up tables in RAM. Recall that we target                           5.2    Experimental Results
to compress vectors such that we do not need to access raw vectors                             5.2.1 Space-Accuracy Trade-Off for Distance Estimation. In this
for re-ranking while still producing reasonable recall. Therefore,                             experiment, we evaluate different quantization methods by using
we focus on evaluating these methods in a moderate compression                                 them for estimating the distances between data vectors and query
rate, i.e., the number of bits per dimension ranges from 1 to 10.                              vectors. For each method, in Figure 3, we vary their number of
When a larger compression rate is used, none of methods can sta-                               bits and plot the curves of the average relative error (left panels,
bly produce over 90% recall without re-ranking. When a smaller                                 lower left is better) and maximum relative error (right panels, lower
compression rate is used, it would be wasteful since 10 bits per                               left is better) to investigate their space-accuracy trade-off. We, in
dimension suffice to produce nearly perfect recall. In addition, in                            particular, focus on the number of bits from ðµ = 1 to ðµ = 10 which
this experiment, as a pre-procession for all the methods, we center                            correspond to a moderate compression rate. Note that ðµ = 8 suffices
the datasets with their global centroid 14 . In the experiments re-                            to produce > 99% recall (Section 5.2.2).
garding the ANN query (Section 5.2.2), based on the experimental                                  According to Figure 3, our method (the green curve) stably
results in Section 5.2.1, we compare our method with the most                                  achieves better accuracy than all the baseline methods on all the
competitive baseline LVQ [1]. We combine our method and LVQ                                    tested datasets when using the same number of bits. Specifically, we
with the IVF index as is discussed in Section 4. In this experiment,                           have the following observations. (1) RabitQ (ext) v.s. SQ and LVQ:
as a pre-procession for all the methods, we center every cluster in                            We observe that when ðµ > 6, under the same number of bits, the
the IVF index with its local centroid. All the methods are optimized                           average relative error of LVQ is consistently larger than ours by
with the SIMD instructions till AVX512.                                                        1.3x-3.1x (the gap of SQâ€™s error from ours is even larger). When
Performance Metrics. In the experiments regarding the trade-off                                ðµ < 6, the gap is even larger. It is worth noting that when ðµ = 1 or
between the space and the accuracy of distance estimation (Sec-                                ðµ = 2, LVQ and SQ hardly produce reasonable accuracy and their
tion 5.2.1), we measure the accuracy with (1) the average relative                             errors are larger than ours by orders of magnitude. Note that for our
                                                                                               method, LVQ and SQ, the computation of distances or inner product
14With the centroid c, the centering operation is to replace every data and query vector       can be implemented in almost identical ways (Section 3.3). This
a with a âˆ’ c.                                                                                  result implies that simply replacing SQ and LVQ in existing systems
                                                                                           8
                                                                                                             SQ                  LVQ               RabitQ (ext)                                         RabitQ (pad)                OPQ              PQ
                                                      MSong                                                                            MSong                                                                               Youtube                                                                          Youtube
                                  2                                                                                                                                                                 2
                             10                                                                                                                                                                10
                                                       out of range (>100%)                                                            out of range (>500%)                                                                 out of range (>100%)                                                             out of range (>500%)
                                                                                                                                                                                                                                                                                        2




                                                                               Maximum Relative Error (%)




                                                                                                                                                                                                                                                      Maximum Relative Error (%)
                                                                                                                                                                                                                                                                                   10
Average Relative Error (%)




                                                                                                                                                                  Average Relative Error (%)
                                                                                                                 2
                                  1                                                                         10                                                                                 10
                                                                                                                                                                                                    1
                             10
                                                                                                                                                                                                                                                                                        1
                                                                                                                 1                                                                                  0                                                                              10
                             10
                                  0
                                                                                                            10                                                                                 10
                                                                                                                                                                                                                                                                                        0
                                  1                                                                                                                                                            10
                                                                                                                                                                                                    1                                                                              10
                             10                                                                             10
                                                                                                                 0

                                                                                                                                                                                                                                                                                        1
                                  2                                                                                                                                                            10
                                                                                                                                                                                                    2                                                                              10
                             10                                                                                  1
                                                                                                            10
                                      1   2   3   4   5   6   7    8    9 10                                         1   2   3    4    5   6   7     8   9 10                                             1   2   3    4    5   6    7    8   9 10                                          1   2   3   4    5   6    7   8   9 10
                                              # of bits per dimension                                                        # of bits per dimension                                                              # of bits per dimension                                                           # of bits per dimension
                                                   OpenAI-1536                                                                    OpenAI-1536                                                                          OpenAI-3072                                                                       OpenAI-3072
                                  2                                                                                                                                                                 2
                             10                                                                                                                                                                10                           out of range (>100%)
                                                       out of range (>100%)                                                            out of range (>500%)                                                                                                                                                  out of range (>500%)
                                                                                                                 2                                                                                                                                                                      2
                                                                                                                                                                                                                                                                                   10
                                                                               Maximum Relative Error (%)




                                                                                                                                                                                                                                                      Maximum Relative Error (%)
                                                                                                            10
Average Relative Error (%)




                                                                                                                                                                  Average Relative Error (%)
                                  1                                                                                                                                                                 1
                             10                                                                                                                                                                10
                                                                                                                 1                                                                                                                                                                      1
                                  0                                                                         10                                                                                      0                                                                              10
                             10                                                                                                                                                                10
                                                                                                                                                                                                                                                                                        0
                                                                                                            10
                                                                                                                 0                                                                                  1                                                                              10
                             10
                                  1                                                                                                                                                            10
                                                                                                                                                                                                                                                                                        1
                                  2                                                                         10
                                                                                                                 1
                                                                                                                                                                                               10
                                                                                                                                                                                                    2                                                                              10
                             10

                                      1   2   3   4   5   6    7   8    9 10                                         1   2   3    4    5   6   7     8   9 10                                             1   2   3    4    5   6    7    8   9 10                                          1   2   3   4    5   6    7   8   9 10
                                              # of bits per dimension                                                        # of bits per dimension                                                              # of bits per dimension                                                           # of bits per dimension
                                                     Word2Vec                                                                       Word2Vec                                                                                GIST                                                                              GIST
                                  2                                                                                                                                                                 2
                             10                                                                                                                                                                10
                                                       out of range (>100%)                                                            out of range (>500%)                                                                 out of range (>100%)                                                             out of range (>500%)
                                                                                                                                                                                                                                                                                        2
                                                                               Maximum Relative Error (%)




                                                                                                                                                                                                                                                      Maximum Relative Error (%)
                                                                                                                 2
                                                                                                                                                                                                                                                                                   10
Average Relative Error (%)




                                                                                                                                                                  Average Relative Error (%)




                                  1                                                                         10                                                                                      1
                             10                                                                                                                                                                10

                                                                                                                 1                                                                                                                                                                      1
                                  0                                                                         10                                                                                 10
                                                                                                                                                                                                    0                                                                              10
                             10
                                                                                                                                                                                                                                                                                        0
                                  1                                                                         10
                                                                                                                 0
                                                                                                                                                                                               10
                                                                                                                                                                                                    1                                                                              10
                             10

                                                                                                                                                                                                    2                                                                                   1
                             10
                                  2
                                                                                                            10
                                                                                                                 1                                                                             10                                                                                  10

                                      1   2   3   4   5   6   7    8    9 10                                         1   2   3    4    5   6   7     8   9 10                                             1   2   3    4    5   6    7    8   9 10                                          1   2   3   4    5   6    7   8   9 10
                                              # of bits per dimension                                                        # of bits per dimension                                                              # of bits per dimension                                                           # of bits per dimension

                                                                        Figure 3: Space-Accuracy Trade-Off for Distance Estimation (Log-Scale).
with our method would stably improve the performance. Moreover,                                                                                                                                 that RaBitQ (pad) is competitive when the number of bits per di-
when a small ðµ is adopted, the improvement would be especially                                                                                                                                  mension is small (e.g., 1 or 2). However, the error of RaBitQ (pad)
significant. This unique advantage enables our method to compute                                                                                                                                also decays slowly. This is because as has been theoretically
                                                                                                                                                                                                                                                        âˆš proved
a fairly accurate distance based on the first bit of the quantization                                                                                                                           in Lemma B.3 [27], its error decays in a trend of ð‘‚ (1/ ðµ Â· ð·). As
codes and prune many candidates, which improves the efficiency. It                                                                                                                              a comparison, the errors of our method, SQ and LVQ decay in an
will be reflected later in Section 5.2.2. (2) PQ and OPQ: We find that                                                                                                                          exponential trend with respect to ðµ.
PQ and OPQ have reasonable accuracy in most datasets when the
number of bits per dimension is small (e.g., 1 or 2). However, when
                                                                                                                                                                                               5.2.2 Time-Accuracy Trade-Off for ANN Query. In this experiment,
the number of bits increases, the errors of these methods do not
                                                                                                                                                                                               we evaluate different quantization methods by using them in com-
decay as fast as our method, SQ or LVQ. In particular, PQ and OPQ
                                                                                                                                                                                               bination with the IVF index for ANN queries. In Figure 4, we plot
fail to outperform SQ and LVQ usually when the number of bits
                                                                                                                                                                                               for the methods their curves of â€œQPSâ€-â€œRecallâ€ (left panels, upper
per dimension is â‰¥ 4. This result has also been observed in the LVQ
                                                                                                                                                                                               right is better) and curves of â€œQPSâ€-â€œAverage Distance Ratioâ€ (right
paper [1]. Moreover, it has also been reported that the efficiency
                                                                                                                                                                                               panels, upper left is better) by varying the number of clusters to
of PQ and OPQ is significantly worse than SQ and LVQ because
                                                                                                                                                                                               probe to investigate their time-accuracy trade-off. In Table 2, we
for computing distances or inner product, they rely on looking up
                                                                                                                                                                                               report the space consumption of different methods for ANN query.
tables in RAM [1, 3]. When FastScan is applied to accelerate the
                                                                                                                                                                                               We note that using the same ðµ, our method has slightly larger space
computation (which requires to set ð‘˜ = 4), the space-accuracy trade-
                                                                                                                                                                                               consumption (e.g., by less than 0.01 GB on million-scale datasets)
off would be even worse [4]. These results indicate that PQ and
                                                                                                                                                                                               than the baseline because our method uses FastScan to estimate
OPQ can hardly be competitive in compressing high-dimensional
                                                                                                                                                                                               distances batch by batch. When a batch is not full, we still allocate
vectors with moderate compression rates. (3) RaBitQ (pad): We find
                                                                                                                                                                                               the memory for a full batch.
                                                                                                                                                                  9
                  RaBitQ (3-bit)         RaBitQ (4-bit)                RaBitQ (5-bit)               RaBitQ (7-bit)                  RaBitQ (8-bit)               RaBitQ (9-bit)                 LVQ (4-bit)                LVQ (8-bit)
                       MSong                                                     MSong                                                                 Youtube                                                           Youtube
      4Ã—                                                 4Ã—                                                              103                                                              103
      2Ã—                                                 2Ã—                                                              6Ã—                                                               6Ã—
                                                                                                                         4Ã—                                                               4Ã—
      103                                                103                                                             2Ã—                                                               2Ã—
      6Ã—                                                 6Ã—
QPS




                                                   QPS




                                                                                                                   QPS




                                                                                                                                                                                    QPS
                                                                                                                         102                                                              102
      4Ã—                                                 4Ã—                                                              6Ã—                                                               6Ã—
                                                                                                                         4Ã—                                                               4Ã—
      2Ã—                                                 2Ã—                                                              2Ã—                                                               2Ã—
      102                                                102                                                             101                                                              101
            80   85       90        95       100               1.000    1.002    1.004       1.006      1.008                  80            85          90         95        100                1.0000 1.0005 1.0010 1.0015 1.0020
                       Recall(%)                                         Average Distance Ratio                                                    Recall(%)                                                  Average Distance Ratio
                      OpenAI-1536                                            OpenAI-1536                                                          OpenAI-3072                                                     OpenAI-3072
      103                                                103                                                             4Ã—                                                               4Ã—
      6Ã—                                                 6Ã—                                                              2Ã—                                                               2Ã—
      4Ã—                                                 4Ã—
      2Ã—                                                 2Ã—                                                              102                                                              102
                                                                                                                         6Ã—                                                               6Ã—
QPS




                                                   QPS




                                                                                                                   QPS




                                                                                                                                                                                    QPS
      102                                                102                                                             4Ã—                                                               4Ã—
      6Ã—                                                 6Ã—
      4Ã—                                                 4Ã—                                                              2Ã—                                                               2Ã—
      2Ã—                                                 2Ã—                                                              101                                                              101
            80   85       90        95       100               1.000     1.002      1.004       1.006                          80            85          90         95        100                 1.000       1.002       1.004    1.006     1.008
                       Recall(%)                                         Average Distance Ratio                                                      Recall(%)                                                Average Distance Ratio
                       Word2Vec                                                Word2Vec                                                                GIST                                                           GIST
      2Ã—                                                 2Ã—                                                              103                                                              103
                                                                                                                         6Ã—                                                               6Ã—
      103                                                103                                                             4Ã—                                                               4Ã—
      6Ã—                                                 6Ã—
      4Ã—                                                 4Ã—                                                              2Ã—                                                               2Ã—
QPS




                                                   QPS




                                                                                                                   QPS




                                                                                                                                                                                    QPS
      2Ã—                                                 2Ã—                                                              102                                                              102
      102                                                102                                                             6Ã—                                                               6Ã—
      6Ã—                                                 6Ã—                                                              4Ã—                                                               4Ã—
      4Ã—                                                 4Ã—
      2Ã—                                                 2Ã—                                                              2Ã—                                                               2Ã—
            80   85       90        95       100               1.000        1.001           1.002          1.003               80            85          90         95        100                 1.000 1.001 1.002 1.003 1.004 1.005
                       Recall(%)                                         Average Distance Ratio                                                      Recall(%)                                                Average Distance Ratio

Figure 4: Time-Accuracy Trade-Off for the ANN Query (Log-Scale), ð¾ = 100. All the methods are combined with the IVF index.


   According to Figure 4, we have the following observations.                                                            other datasets which have lower dimensionality would be smaller.
(1) Time-accuracy trade-off: We observe that with the same num-                                                          Based on the results in Figure 4 and Table 3, we observe that when
ber of bits (4-bit and 8-bit), our method outperforms LVQ in terms                                                       setting ðµ = 5 and ðµ = 7, it suffices to produce >95% and >99% recall
of time-accuracy trade-off. It is worth highlighting that the im-                                                        respectively, and the quantization can finish in a few minutes. Thus,
provement in efficiency under the same recall is significant since                                                       the time for quantizing data vectors in the index phase is not a
our method can prune many candidates with the estimated dis-                                                             bottleneck in practical usage.
tances based on the first bits of the quantization codes. In addition,
the estimation can be implemented with FastScan [4] which is
highly efficient (Section 4.2). (2) Recall: We find that 4-bit, 5-bit                                                                        RaBitQ (3-bit)    RaBitQ (5-bit)                           RaBitQ (8-bit)         LVQ (4-bit)
                                                                                                                                             RaBitQ (4-bit)    RaBitQ (7-bit)                           RaBitQ (9-bit)         LVQ (8-bit)
and 7-bit quantization suffices to produce over 90%, 95% and 99%
                                                                                                                                                       MSMARCO                                                           MSMARCO
recall on all the tested datasets respectively without re-ranking.                                                              4Ã—                                                              4Ã—
                                                                                                                                2Ã—                                                              2Ã—
(3) Average distance ratio: We observe that the average distance                                                                  102                                                             102
                                                                                                                                64 Ã—Ã—                                                           64 Ã—Ã—
ratio produced by 5-bit quantization of our method is nearly perfect                                                             2Ã—                                                              2Ã—
                                                                                                                          QPS




                                                                                                                                                                                          QPS




across all the datasets while it cannot achieve perfect recall (i.e., it                                                          101                                                             101
                                                                                                                                 64 Ã—
                                                                                                                                    Ã—                                                            64 Ã—
                                                                                                                                                                                                    Ã—
usually produces 97% recall only). This indicates that the retrieved                                                              2Ã—                                                              2Ã—
data vectors have their distances from the query extremely close                                                                  100                                                             100
                                                                                                                                        80        85          90         95         100            1.0000 1.0025 1.0050 1.0075 1.0100 1.0125
to those of the true NNs. (4) Results on datasets from OpenAI: We                                                                                         Recall(%)                                              Average Distance Ratio
find that the ANN algorithm with our method on these datasets                                                                                Figure 5: Verification Study for Scalability.
is highly robust, e.g., 3-bit quantization suffices to produce > 95%
recall.                                                                                                                  5.2.4 Verifying the Scalability. In this section, we study the scal-
                                                                                                                         ability of our method on the MSMARCO dataset with about 100
5.2.3 Time of Quantizing in the Indexing Phase. In this section,                                                         millions of 1,024-dimensional data vectors. We report that the quan-
we report the time for quantizing the vectors in the index phase                                                         tization of the dataset with ðµ = 9 can be finished in around 2 hours,
based on different ðµâ€™s. This time includes both (1) the time for                                                         which is not a bottleneck (as a comparison, building the IVF index
multiplying by ð‘ƒ âˆ’1 the data vectors and (2) the time for computing                                                      for the dataset takes more than 1 day). Figure 5 reports the time-
the quantization codes, i.e., Algorithm 1. Due to the limit of space,                                                    accuracy trade-off for ANN queries. It shows that our method still
we only report the results on the dataset OpenAI-3072 which has                                                          achieves consistently better time-accuracy trade-off compared with
the highest dimensionality. It is clear that the quantization time for                                                   LVQ. In particular, with our method, using ðµ = 4 suffices to produce
                                                                                                                10
                                           Table 2: Space Consumption for ANN Query (GB).
                           Raw Vectors     RaBitQ-3 RaBitQ-4 RaBitQ-5 RaBitQ-7 RaBitQ-8                                                    RaBitQ-9                            LVQ-4       LVQ-8
            MSong              1.56           0.18      0.23     0.28       0.39    0.44                                                     0.49                                0.22       0.41
            Youtube            3.82           0.40      0.52     0.64       0.87    0.99                                                     1.11                                0.51       0.98
          OpenAI-1537          5.72           0.59      0.77     0.95       1.31    1.48                                                     1.66                                0.75       1.47
          OpenAI-3072         11.44          1.19       1.54    1.90       2.62     2.97                                                     3.33                               1.49        2.92
           Word2Vec            1.12           0.13      0.17     0.21       0.28    0.32                                                     0.35                                0.16       0.30
             GIST              3.58           0.37      0.48     0.60       0.82    0.93                                                     1.04                                0.48       0.92
          MSMARCO            433.47          43.41     56.94    70.48      97.54   111.07                                                   124.61                              56.86      110.99
                                                                                                                  99.9% Quantile of Empirical Errors                           = c 2 B/ D, c = 5.75
Table 3: Time for quantizing the data vectors in OpenAI-3072                                                          D = 1000                                                             B=4
(âˆ¼ 106 vectors) with 96 threads/48 cores.                                                                                                                          0.035
                                                                                               0.08
             ðµ       1     2      3       4       5                                                                                                                0.030




                                                                              Absolute Error




                                                                                                                                                  Absolute Error
          Time (s) 43.8 47.7 52.6       58.1    64.6                                           0.06                                                                0.025
             ðµ       6     7      8       9      10                                            0.04                                                                0.020
          Time (s) 76.0 98.3 143.7 233.2 418.1                                                                                                                     0.015
                                                                                               0.02
                                                                                                                                                                   0.010
>95% recall without re-ranking. In this case, the space consumption                            0.00                                                                0.005
of our method is 56.94 GB while the raw dataset takes 433.47 GB.                                      1   2   3   4    5       6   7   8   9 10                            0       1000     2000      3000   4000
                                                                                                                           B                                                                 D
                                                                              Figure 7: Measure the Constants in the Empirical Formula.

                                                                              curve). In the right panel, on the other hand, we fix ðµ = 4 and plot
                                                                              the curve of the 99.9% quantile of the empirical errors with respect
                                                                              to ð· (the red curve).
                                                                                                âˆš    Note that our empirical formula is in the form
                                                                              of ðœ– < ð‘ðœ– Â· 2 âˆ’ðµ / ð· (see Section 3.4). We measure
                                                                                                                              âˆš the constant ð‘ðœ– by
                                                                              tuning ð‘ðœ– such that the curve of ðœ– = ð‘ðœ– Â· 2 âˆ’ðµ / ð· (the green curve)
                                                                              is higher than the curve of the 99.9% quantile of the empirical errors.
                                                                              The result shows that when ð‘ðœ– = 5.75, the curves match well.

                                                                              6                  RELATED WORK
                                                                              Quantization. A substantial body of literature on the quanti-
        Figure 6: Verification Study for Unbiasedness.                        zation of high-dimensional vectors exists across various fields,
5.2.5 Verifying the Unbiasedness. In this section, we verify that             including machine learning, computer vision, and data manage-
based on our new codebook with more vectors than those of the                 ment [1, 8, 22, 29, 30, 37, 38, 46, 54, 68, 68, 77, 81]. We refer readers
original RaBitQ method, the estimator is still unbiased. Due to the           to comprehensive surveys and textbooks [19, 47, 62, 70, 72, 80]. The
limit of space, we only present the results for ðµ = 2, 3 in two panels        existing studies about quantization can be divided into roughly
of Figure 6 respectively. In each panel, we collect around 107 pairs          two threads: (1) PQ and its variant [8, 29, 37, 46, 54] and (2) SQ
of the estimated squared distances and the true squared distances             and its variants [1, 22, 77]. We note that although the family of
(between the first 10 query vectors and about 106 data vectors in             PQ and the family of SQ have highly diversified schemes, they
the OpenAI-3072 dataset) which are normalized with the maximum                can be presented in a unified framework: (1) in the index phase,
true squared distances. To verify the unbiasedness, we fit these pairs        they construct a quantization codebook and for each data vector,
with linear regression and plot the result with the black dashed line.        they find the nearest vector in it as the quantized data vector; and
Note that in all panels, this line has the slope of 1 and the y-axis          (2) in the query phase, they estimate the distance between a data
intercept of 0. This indicates that our method provides unbiased              vector and a query vector based on the quantized data vector. De-
distance estimation.                                                          spite this, we observe that in existing literature [17, 29, 37, 45, 47],
                                                                              PQ and SQ were seldom compared to each other 15 . Indeed, PQ
5.2.6 Measuring the Empirical Formula among ðœ–, ð· and ðµ. In this
                                                                              and its variants are used mostly in the scenarios with large com-
section, we measure the constants in the empirical formula which
                                                                              pression rates [17, 29, 37, 45, 47, 70, 72, 81] while SQ and its vari-
presents the relationship among the absolute error of estimating
                                                                              ants are used mostly in the scenarios with moderate compression
inner product between unit vectors, ðµ and ð· (see Section 3.4). In
                                                                              rates [1, 17, 71, 79]. According to our experimental results in Sec-
particular, for a pair of ðµ and ð·, we randomly sample 5 Ã— 106 pairs
                                                                              tion 5.2.1, we find that with moderate compression rates (e.g., quan-
of unit vectors (i.e., each vector is sampled from standard Gaussian
                                                                              tizing the vectors with â‰¥ 4 bits per dimension), PQ does not out-
distribution and is normalized then), apply our method to estimate
                                                                              perform the classical SQ on many datasets. In addition, as has been
their inner product and collect the error of the estimation. In the
left panel of Figure 7, we fix ð· = 1000 and plot the curve of the             15 The LVQ paper [1] compares PQ and SQ and finds that PQ does not have competitive
99.9% quantile of the empirical errors with respect to ðµ (the red             performance with a moderate compression rate.
                                                                         11
reported [1], PQ is significantly slower than SQ in distance compu-                           asymptotically optimal in terms of the trade-off between the num-
tation when the same number of bits are used. On the other hand,                              ber of dimensions and the error bound [42]. For more applications
with relatively large compression rates (e.g., quantizing the vectors                         and theoretical conclusions, we refer readers to a comprehensive
with 1 or 2 bits per dimension), SQ can hardly produce reasonable                             introduction [23]. Moreover, several studies further investigate on
accuracy. This may help explain why PQ and SQ were usually re-                                the trade-off between the number of bits for representing a vector
garded as two separate lines of studies, i.e., each of them is capable                        and the error bound of inner product/distance estimation [2, 35, 41].
of a certain scenario only. In contrast, in this study, we develop a                          In particular, they prove that compressing a vector into a short code
method which excels in both scenarios. With the compression rates                             with ð‘‚ (ðœ– âˆ’2 log(1/ð›¿)) bits suffices to guarantee an error bound of ðœ–.
from 3x to 32x, our method stably outperforms all the methods                                 As a comparison, trivially applying JL Lemma and quantizing the
empirically (Section 5.2.1). Moreover, it achieves the asymptotic                             real value of each dimension with SQ requires ð‘‚ (log ðœ–1 ) bits per di-
optimality in theory (Section 3.4). In addition, it is worth noting that                      mension [2]. Furthermore, it is observed in [2], for the first time, that
PQ and its variants are also used with a compression rate which                               when the required accuracy is high (ðœ– is small, ðœ– âˆ’2 log(1/ð›¿) > ð·),
is even larger than 32x. However, in this case, without re-ranking,                           the aforementioned result can be further improved (see Section 3.1).
they can only produce poor recall [17, 29, 37, 45, 47, 70, 72, 81] (e.g.,                     However, as has been discussed in Section 3.4, the algorithmic proof
<80%), which deviates from the target of this study, i.e., achieving                          in this study is hardly practically applicable.
high recall without storing the raw vectors in RAM for re-ranking.
For better comprehensiveness, we include the discussion on the                                7    CONCLUSION
way of using our method with a larger compression rate in Section 7
                                                                                              In conclusion, in this study, we propose a novel quantization algo-
and leave more detailed study as future work.
                                                                                              rithm which extends the RaBitQ method. It supports to compress
ANN Query. ANN query is a key component of high-dimensional                                   high-dimensional vectors with a moderate compression rate such
vector data management and has been widely supported by real-                                 that it produces promising recall without re-ranking. The method
world systems [31, 49, 52, 71, 79]. Among the existing algo-                                  has consistent advantages in both empirical accuracy and theoret-
rithms [12, 15, 24, 29, 37, 43, 44, 76, 78], the partition-based method                       ical guarantees over SQ and its variants. In addition, it supports
IVF and the graph-based method HNSW have been deployed to the                                 efficient distance comparisons by first estimating a distance based
widest extent [44, 52, 71, 79]. For more methods, we refer readers                            on the first bits of the quantization codes, which helps prune many
to recent tutorials [18, 53, 61], surveys [7, 52, 57, 75] and bench-                          candidates from the distance computation based on the full codes.
marks [6, 16, 43, 74]. Besides the ANN query, there have been grow-                           Extensive experiments verify its effectiveness in practice and the
ing interests in many advanced queries related to ANN [49, 52, 71].                           alignment between the empirical performance and the theoretical
For example, in real-world systems, besides vectors, a data object                            analysis.
usually involves several attributes such as labels, numbers and                                  We would like to discuss on the following applications and ex-
strings. It is often the case that users target to find the data objects                      tensions. (1) Like the original RaBitQ, the method can be adapted to
which satisfy some constraints on the attributes and have their vec-                          support the estimation of inner product and cosine similarity (see
tor nearest to the userâ€™s query vector. These questions are usually                           [27]). (2) When targeting to compress vectors with a compression
referred to as attribute-filtering ANN query [49, 52, 55, 71, 79, 83].                        rate of >32x (i.e., we use fewer than 1 bit per dimension for quantiz-
Besides, due to the recent development in information retrieval,                              ing a vector), we could first apply random projection for dimension
there has also been a trend in multi-vector search [40, 52, 63]. In par-                      reduction and apply the original RaBitQ method for binarization.
ticular, these studies map a document to multiple high-dimensional                            The details are left in Appendix C due to the page limit. (3) Our
vectors. During querying, they also map a query document to mul-                              method may bring benefits to other scenarios of ANN search (e.g.,
tiple vectors and search for the most relevant documents via a new                            we may put the first bits of the quantization codes in RAM, leave
similarity metric called MaxSim, which is aggregated from the inner                           the remaining bits in SSDs and conduct distance comparisons like
products between query and data vectors [40, 63]. We note that in                             what we do in Section 4.2). (4) Quantization is also a fundamental
this paper, we present a method of vector compression and support                             component in machine learning systems. The practicality and opti-
to estimate inner product and squared Euclidean distances unbi-                               mality of RaBitQ implies its potential of optimizing neural network
asedly based on the compressed vectors. Our method can be used                                inferencing and training.
in these tasks for vector compression and distances/inner-product
estimation seamlessly.                                                                        REFERENCES
Random Projection. Random projection is a fundamental tech-                                    [1] Cecilia Aguerrebere, Ishwar Singh Bhati, Mark Hildebrand, Mariano Tepper, and
                                                                                                   Theodore Willke. 2023. Similarity Search in the Blink of an Eye with Compressed
nique that has wide applications [10, 14, 15, 26, 27, 33, 66, 67]. The                             Indices. Proc. VLDB Endow. 16, 11 (aug 2023), 3433â€“3446. https://doi.org/10.
effectiveness of random projection is closely related to the seminal                               14778/3611479.3611537
Johnson-Lindenstrauss (JL) Lemma [39]. In particular, JL Lemma                                 [2] Noga Alon and Boâ€™az Klartag. 2017. Optimal Compression of Approximate Inner
                                                                                                   Products and Dimension Reduction. In 2017 IEEE 58th Annual Symposium on
states that projecting a vector onto the space of ð‘‚ (ðœ– âˆ’2 log(1/ð›¿))                                Foundations of Computer Science (FOCS). 639â€“650. https://doi.org/10.1109/FOCS.
dimensions suffices to provide an error bound of ðœ– with the proba-                                 2017.65
                                                                                               [3] Fabien AndrÃ©, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. 2015. Cache
bility of at least 1 âˆ’ð›¿ 16 . A theoretical study proves that JL Lemma is                           Locality is Not Enough: High-Performance Nearest Neighbor Search with Product
                                                                                                   Quantization Fast Scan. Proc. VLDB Endow. 9, 4 (dec 2015), 288â€“299. https:
                                                                                                   //doi.org/10.14778/2856318.2856324
16 The error bound includes a multiplicative error bound of Euclidean distances and an
                                                                                               [4] Fabien AndrÃ©, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. 2017. Accel-
additive error bound of inner product.                                                             erated Nearest Neighbor Search with Quick ADC. In Proceedings of the 2017
                                                                                         12
     ACM on International Conference on Multimedia Retrieval (Bucharest, Romania)                        Search. Proc. ACM Manag. Data 2, 3, Article 167 (may 2024), 27 pages. https:
     (ICMR â€™17). Association for Computing Machinery, New York, NY, USA, 159â€“166.                        //doi.org/10.1145/3654970
     https://doi.org/10.1145/3078971.3078992                                                        [28] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi
 [5] Fabien AndrÃ©, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. 2021. Quicker                         Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented
     ADC : Unlocking the Hidden Potential of Product Quantization With SIMD. IEEE                        Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]
     Transactions on Pattern Analysis and Machine Intelligence 43, 5 (2021), 1666â€“1677.                  https://arxiv.org/abs/2312.10997
     https://doi.org/10.1109/TPAMI.2019.2952606                                                     [29] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product
 [6] Martin AumÃ¼ller, Erik Bernhardsson, and Alexander Faithfull. 2020. ANN-                             quantization for approximate nearest neighbor search. In Proceedings of the IEEE
     Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algo-                              Conference on Computer Vision and Pattern Recognition. 2946â€“2953.
     rithms. Inf. Syst. 87, C (jan 2020), 13 pages. https://doi.org/10.1016/j.is.2019.02.006        [30] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. 2013.
 [7] Martin AumÃ¼ller and Matteo Ceccarello. 2023. Recent Approaches and Trends in                        Iterative Quantization: A Procrustean Approach to Learning Binary Codes for
     Approximate Nearest Neighbor Search, with Remarks on Benchmarking. Data                             Large-Scale Image Retrieval. IEEE Transactions on Pattern Analysis and Machine
     Engineering (2023), 89.                                                                             Intelligence 35, 12 (2013), 2916â€“2929. https://doi.org/10.1109/TPAMI.2012.193
 [8] Artem Babenko and Victor Lempitsky. 2014. Additive Quantization for Extreme                    [31] Rentong Guo, Xiaofan Luan, Long Xiang, Xiao Yan, Xiaomeng Yi, Jigao Luo,
     Vector Compression. In 2014 IEEE Conference on Computer Vision and Pattern                          Qianya Cheng, Weizhi Xu, Jiarui Luo, Frank Liu, Zhenshan Cao, Yanliang Qiao,
     Recognition. 931â€“938. https://doi.org/10.1109/CVPR.2014.124                                         Ting Wang, Bo Tang, and Charles Xie. 2022. Manu: A Cloud Native Vector
 [9] Object Box. 2024. Object Box. https://objectbox.io/vector-database-for-ondevice-                    Database Management System. Proc. VLDB Endow. 15, 12 (aug 2022), 3548â€“3561.
     ai/.                                                                                                https://doi.org/10.14778/3554821.3554843
[10] Moses S. Charikar. 2002. Similarity Estimation Techniques from Rounding Algo-                  [32] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and
     rithms. In Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of                        Sanjiv Kumar. 2020. Accelerating Large-Scale Inference with Anisotropic Vector
     Computing (Montreal, Quebec, Canada) (STOC â€™02). Association for Computing                          Quantization. In Proceedings of the 37th International Conference on Machine
     Machinery, New York, NY, USA, 380â€“388. https://doi.org/10.1145/509907.509965                        Learning (ICMLâ€™20). JMLR.org, Article 364, 10 pages.
[11] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li,                      [33] Qiang Huang, Jianlin Feng, Yikai Zhang, Qiong Fang, and Wilfred Ng. 2015.
     Mao Yang, and Jingdong Wang. 2021. SPANN: Highly-efficient Billion-scale                            Query-aware locality-sensitive hashing for approximate nearest neighbor search.
     Approximate Nearest Neighbor Search. In 35th Conference on Neural Information                       Proceedings of the VLDB Endowment 9, 1 (2015), 1â€“12.
     Processing Systems (NeurIPS 2021).                                                             [34] Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards
[12] Paolo Ciaccia, Marco Patella, and Pavel Zezula. 1997. M-Tree: An Efficient                          removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM
     Access Method for Similarity Search in Metric Spaces. In Proceedings of the 23rd                    symposium on Theory of computing. 604â€“613.
     International Conference on Very Large Data Bases (VLDB â€™97). Morgan Kaufmann                  [35] Piotr Indyk and Tal Wagner. 2022. Optimal (Euclidean) Metric Compression.
     Publishers Inc., San Francisco, CA, USA, 426â€“435.                                                   SIAM J. Comput. 51, 3 (2022), 467â€“491. https://doi.org/10.1137/20M1371324
[13] Couchbase. 2024. Vector Search at the Edge with Couchbase Mobile. https://                          arXiv:https://doi.org/10.1137/20M1371324
     www.couchbase.com/blog/vector-search-at-the-edge-with-couchbase-mobile/.                       [36] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar
[14] Sanjoy Dasgupta and Yoav Freund. 2008. Random projection trees and low                              Krishnawamy, and Rohan Kadekodi. 2019. DiskANN: Fast Accurate Billion-point
     dimensional manifolds. In Proceedings of the fortieth annual ACM symposium on                       Nearest Neighbor Search on a Single Node. In Advances in Neural Information
     Theory of computing. 537â€“546.                                                                       Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc,
[15] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. 2004. Locality-                   E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.
     sensitive hashing scheme based on p-stable distributions. In Proceedings of the                     neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf
     twentieth annual symposium on Computational geometry. 253â€“262.                                 [37] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization
[16] Magdalen Dobson, Zheqi Shen, Guy E Blelloch, Laxman Dhulipala, Yan Gu,                              for nearest neighbor search. IEEE transactions on pattern analysis and machine
     Harsha Vardhan Simhadri, and Yihan Sun. 2023. Scaling Graph-Based ANNS                              intelligence 33, 1 (2010), 117â€“128.
     Algorithms to Billion-Size Datasets: A Comparative Analysis. arXiv preprint                    [38] Wenqi Jiang, Shigang Li, Yu Zhu, Johannes De Fine Licht, Zhenhao He, Runbin
     arXiv:2305.04359 (2023).                                                                            Shi, Cedric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten Hoefler, and
[17] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,                      Gustavo Alonso. 2023. Co-design Hardware and Algorithm for Vector Search.
     Pierre-Emmanuel MazarÃ©, Maria Lomeli, Lucas Hosseini, and HervÃ© JÃ©gou. 2024.                        In Proceedings of the International Conference for High Performance Computing,
     The Faiss library. arXiv:2401.08281 [cs.LG] https://arxiv.org/abs/2401.08281                        Networking, Storage and Analysis (Denver, CO, USA) (SC â€™23). Association for
[18] Karima Echihabi, Kostas Zoumpatianos, and Themis Palpanas. 2021. New Trends                         Computing Machinery, New York, NY, USA, Article 87, 15 pages. https://doi.
     in High-D Vector Similarity Search: Al-Driven, Progressive, and Distributed. Proc.                  org/10.1145/3581784.3607045
     VLDB Endow. 14, 12 (jul 2021), 3198â€“3201. https://doi.org/10.14778/3476311.                    [39] William B Johnson and Joram Lindenstrauss. 1984. Extensions of Lipschitz
     3476407                                                                                             mappings into a Hilbert space 26. Contemporary mathematics 26 (1984), 28.
[19] Karima Echihabi, Kostas Zoumpatianos, Themis Palpanas, and Houda Benbrahim.                    [40] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage
     2018. The Lernaean Hydra of Data Series Similarity Search: An Experimental                          search via contextualized late interaction over bert. In Proceedings of the 43rd
     Evaluation of the State of the Art. Proc. VLDB Endow. 12, 2 (oct 2018), 112â€“127.                    International ACM SIGIR conference on research and development in Information
     https://doi.org/10.14778/3282495.3282498                                                            Retrieval. 39â€“48.
[20] Elastic. 2024. Elastic. https://www.elastic.co/enterprise-search/vector-search.                [41] Eyal Kushilevitz, Rafail Ostrovsky, and Yuval Rabani. 1998. Efficient search for
[21] Faiss. 2023. Faiss. https://github.com/facebookresearch/faiss.                                      approximate nearest neighbor in high dimensional spaces. In Proceedings of the
[22] Hakan Ferhatosmanoglu, Ertem Tuncel, Divyakant Agrawal, and Amr El Abbadi.                          Thirtieth Annual ACM Symposium on Theory of Computing (Dallas, Texas, USA)
     2000. Vector Approximation Based Indexing for Non-Uniform High Dimensional                          (STOC â€™98). Association for Computing Machinery, New York, NY, USA, 614â€“623.
     Data Sets. In Proceedings of the Ninth International Conference on Information                      https://doi.org/10.1145/276698.276877
     and Knowledge Management (McLean, Virginia, USA) (CIKM â€™00). Association for                   [42] Kasper Green Larsen and Jelani Nelson. 2017. Optimality of the Johnson-
     Computing Machinery, New York, NY, USA, 202â€“209. https://doi.org/10.1145/                           Lindenstrauss lemma. In 2017 IEEE 58th Annual Symposium on Foundations of
     354756.354820                                                                                       Computer Science (FOCS). IEEE, 633â€“638.
[23] Casper Benjamin Freksen. 2021. An Introduction to Johnson-Lindenstrauss                        [43] Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li, Wenjie Zhang, and
     Transforms. CoRR abs/2103.00564 (2021). arXiv:2103.00564 https://arxiv.org/abs/                     Xuemin Lin. 2019. Approximate nearest neighbor search on high dimensional
     2103.00564                                                                                          dataâ€”experiments, analyses, and improvement. IEEE Transactions on Knowledge
[24] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. 2019. Fast Approximate                             and Data Engineering 32, 8 (2019), 1475â€“1488.
     Nearest Neighbor Search with the Navigating Spreading-out Graph. Proc. VLDB                    [44] Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate
     Endow. 12, 5 (jan 2019), 461â€“474. https://doi.org/10.14778/3303753.3303754                          Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.
[25] Junhao Gan, Jianlin Feng, Qiong Fang, and Wilfred Ng. 2012. Locality-Sensitive                      IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 4 (2020), 824â€“
     Hashing Scheme Based on Dynamic Collision Counting. In Proceedings of the                           836. https://doi.org/10.1109/TPAMI.2018.2889473
     2012 ACM SIGMOD International Conference on Management of Data (Scottsdale,                    [45] Julieta Martinez, Joris Clement, Holger H. Hoos, and James J. Little. 2016. Re-
     Arizona, USA) (SIGMOD â€™12). Association for Computing Machinery, New York,                          visiting Additive Quantization. In Computer Vision â€“ ECCV 2016, Bastian Leibe,
     NY, USA, 541â€“552. https://doi.org/10.1145/2213836.2213898                                           Jiri Matas, Nicu Sebe, and Max Welling (Eds.). Springer International Publishing,
[26] Jianyang Gao and Cheng Long. 2023. High-Dimensional Approximate Nearest                             Cham, 137â€“153.
     Neighbor Search: With Reliable and Efficient Distance Comparison Operations.                   [46] Julieta Martinez, Shobhit Zakhmi, Holger H. Hoos, and James J. Little. 2018.
     Proc. ACM Manag. Data 1, 2, Article 137 (jun 2023), 27 pages. https://doi.org/10.                   LSQ++: Lower Running Time and Higher Recall in Multi-Codebook Quantization.
     1145/3589282                                                                                        In Computer Vision â€“ ECCV 2018: 15th European Conference, Munich, Germany,
[27] Jianyang Gao and Cheng Long. 2024. RaBitQ: Quantizing High-Dimensional                              September 8-14, 2018, Proceedings, Part XVI (Munich, Germany). Springer-Verlag,
     Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor                             Berlin, Heidelberg, 508â€“523. https://doi.org/10.1007/978-3-030-01270-0_30
                                                                                               13
[47] Yusuke Matsui, Yusuke Uchida, HervÃ© JÃ©gou, and Shinâ€™ichi Satoh. 2018. A Survey                â€™21). Association for Computing Machinery, New York, NY, USA, 2614â€“2627.
     of Product Quantization. ITE Transactions on Media Technology and Applications                https://doi.org/10.1145/3448016.3457550
     6, 1 (2018), 2â€“10.                                                                       [72] Jingdong Wang, Ting Zhang, jingkuan song, Nicu Sebe, and Heng Tao Shen. 2018.
[48] Milvus. 2024. Run Milvus Lite Locally. https://milvus.io/docs/install-overview.               A Survey on Learning to Hash. IEEE Transactions on Pattern Analysis and Machine
     md.                                                                                           Intelligence 40, 4 (2018), 769â€“790. https://doi.org/10.1109/TPAMI.2017.2699960
[49] Jason Mohoney, Anil Pacaci, Shihabur Rahman Chowdhury, Ali Mousavi, Ihab F.              [73] Mengzhao Wang, Weizhi Xu, Xiaomeng Yi, Songlin Wu, Zhangyang Peng, Xi-
     Ilyas, Umar Farooq Minhas, Jeffrey Pound, and Theodoros Rekatsinas. 2023. High-               angyu Ke, Yunjun Gao, Xiaoliang Xu, Rentong Guo, and Charles Xie. 2024.
     Throughput Vector Similarity Search in Knowledge Graphs. Proc. ACM Manag.                     Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-
     Data 1, 2, Article 197 (jun 2023), 25 pages. https://doi.org/10.1145/3589777                  Dimensional Vector Similarity Search on Data Segment. Proc. ACM Manag.
[50] Marius Muja and David G Lowe. 2014. Scalable nearest neighbor algorithms                      Data 2, 1, Article 14 (mar 2024), 27 pages. https://doi.org/10.1145/3639269
     for high dimensional data. IEEE transactions on pattern analysis and machine             [74] Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang. 2021. A Compre-
     intelligence 36, 11 (2014), 2227â€“2240.                                                        hensive Survey and Experimental Comparison of Graph-Based Approximate
[51] OpenAI. 2024. New embedding models and API updates. https://openai.com/                       Nearest Neighbor Search. Proc. VLDB Endow. 14, 11 (jul 2021), 1964â€“1978.
     index/new-embedding-models-and-api-updates/.                                                  https://doi.org/10.14778/3476249.3476255
[52] James Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database                [75] Zeyu Wang, Peng Wang, Themis Palpanas, and Wei Wang. 2023. Graph-and
     management systems. The VLDB Journal (07 2024), 1â€“25. https://doi.org/10.                     Tree-based Indexes for High-dimensional Vector Similarity Search: Analyses,
     1007/s00778-024-00864-x                                                                       Comparisons, and Future Directions. Data Engineering (2023), 3â€“21.
[53] James Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Vector Database Man-                 [76] Zeyu Wang, Haoran Xiong, Zhenying He, Peng Wang, and Wei wang. 2024. Dis-
     agement Techniques and Systems. In Companion of the 2024 International Con-                   tance Comparison Operators for Approximate Nearest Neighbor Search: Explo-
     ference on Management of Data (Santiago AA, Chile) (SIGMOD/PODS â€™24). As-                     ration and Benchmark. arXiv:2403.13491 [cs.DB] https://arxiv.org/abs/2403.13491
     sociation for Computing Machinery, New York, NY, USA, 597â€“604. https:                    [77] Roger Weber, Hans-JÃ¶rg Schek, and Stephen Blott. 1998. A Quantitative Analysis
     //doi.org/10.1145/3626246.3654691                                                             and Performance Study for Similarity-Search Methods in High-Dimensional
[54] John Paparrizos, Ikraduya Edian, Chunwei Liu, Aaron J. Elmore, and Michael J.                 Spaces. In Proceedings of the 24rd International Conference on Very Large Data
     Franklin. 2022. Fast Adaptive Similarity Search through Variance-Aware Quan-                  Bases (VLDB â€™98). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,
     tization. In 2022 IEEE 38th International Conference on Data Engineering (ICDE).              194â€“205.
     2969â€“2983. https://doi.org/10.1109/ICDE53745.2022.00268                                  [78] Mingyu Yang, Wentao Li, Jiabao Jin, Xiaoyao Zhong, Xiangyu Wang, Zhitao
[55] Liana Patel, Peter Kraft, Carlos Guestrin, and Matei Zaharia. 2024. ACORN:                    Shen, Wei Jia, and Wei Wang. 2024. Effective and General Distance Computation
     Performant and Predicate-Agnostic Search Over Vector Embeddings and Struc-                    for Approximate Nearest Neighbor Search. arXiv:2404.16322 [cs.DB] https:
     tured Data. Proc. ACM Manag. Data 2, 3, Article 120 (may 2024), 27 pages.                     //arxiv.org/abs/2404.16322
     https://doi.org/10.1145/3654923                                                          [79] Wen Yang, Tao Li, Gai Fang, and Hong Wei. 2020. PASE: PostgreSQL Ultra-High-
[56] Marco Patella and Paolo Ciaccia. 2008. The Many Facets of Approximate Similar-                Dimensional Approximate Nearest Neighbor Search Extension. In Proceedings of
     ity Search. In First International Workshop on Similarity Search and Applications             the 2020 ACM SIGMOD International Conference on Management of Data (Portland,
     (sisap 2008). 10â€“21. https://doi.org/10.1109/SISAP.2008.18                                    OR, USA) (SIGMOD â€™20). Association for Computing Machinery, New York, NY,
[57] Marco Patella and Paolo Ciaccia. 2009. Approximate Similarity Search: A Multi-                USA, 2241â€“2253. https://doi.org/10.1145/3318464.3386131
     Faceted Problem. J. of Discrete Algorithms 7, 1 (mar 2009), 36â€“48. https://doi.          [80] Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, and Michal Batko. 2010. Simi-
     org/10.1016/j.jda.2008.09.014                                                                 larity Search: The Metric Space Approach (1st ed.). Springer Publishing Company,
[58] pgvector. 2024. pgvector. https://github.com/pgvector/pgvector.                               Incorporated.
[59] pgvector.rs. 2024. pgvector.rs. https://pgvecto.rs/.                                     [81] Ting Zhang, Chao Du, and Jingdong Wang. 2014. Composite Quantization
[60] Qdrant. 2024. Qdrant. https://qdrant.tech/.                                                   for Approximate Nearest Neighbor Search. In Proceedings of the 31st Interna-
[61] Jianbin Qin, Wei Wang, Chuan Xiao, Ying Zhang, and Yaoshu Wang. 2021. High-                   tional Conference on Machine Learning (Proceedings of Machine Learning Research,
     Dimensional Similarity Query Processing for Data Science. In Proceedings of the               Vol. 32), Eric P. Xing and Tony Jebara (Eds.). PMLR, Bejing, China, 838â€“846.
     27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Virtual                    https://proceedings.mlr.press/v32/zhangd14.html
     Event, Singapore) (KDD â€™21). Association for Computing Machinery, New York,              [82] Zilliz. 2023. Pyglass - Graph Library for Approximate Similarity Search. https:
     NY, USA, 4062â€“4063. https://doi.org/10.1145/3447548.3470811                                   //github.com/zilliztech/pyglass. Accessed: 17 Apr, 2024.
[62] Hanan Samet. 2005. Foundations of Multidimensional and Metric Data Structures            [83] Chaoji Zuo, Miao Qiao, Wenchao Zhou, Feifei Li, and Dong Deng. 2024. SeRF:
     (The Morgan Kaufmann Series in Computer Graphics and Geometric Modeling).                     Segment Graph for Range-Filtering Approximate Nearest Neighbor Search. Proc.
     Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.                                      ACM Manag. Data 2, 1, Article 69 (mar 2024), 26 pages. https://doi.org/10.1145/
[63] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei                 3639324
     Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight late
     interaction. arXiv preprint arXiv:2112.01488 (2021).
[64] SingleStore. 2024. SingleStore. https://www.singlestore.com/built-in-vector-
     database/.
[65] Yongye Su, Yinqi Sun, Minjia Zhang, and Jianguo Wang. 2024. Vexless: A Server-
     less Vector Data Management System Using Cloud Functions. Proc. ACM Manag.
     Data 2, 3, Article 187 (may 2024), 26 pages. https://doi.org/10.1145/3654990
[66] Yifang Sun, Wei Wang, Jianbin Qin, Ying Zhang, and Xuemin Lin. 2014. SRS:
     solving c-approximate nearest neighbor queries in high dimensional euclidean
     space with a tiny index. Proceedings of the VLDB Endowment (2014).
[67] Yufei Tao, Ke Yi, Cheng Sheng, and Panos Kalnis. 2010. Efficient and accu-
     rate nearest neighbor and closest pair search in high-dimensional space. ACM
     Transactions on Database Systems (TODS) 35, 3 (2010), 1â€“46.
[68] Ertem Tuncel, Hakan Ferhatosmanoglu, and Kenneth Rose. 2002. VQ-Index: An
     Index Structure for Similarity Searching in Multimedia Databases. In Proceedings
     of the Tenth ACM International Conference on Multimedia (Juan-les-Pins, France)
     (MULTIMEDIA â€™02). Association for Computing Machinery, New York, NY, USA,
     543â€“552. https://doi.org/10.1145/641007.641117
[69] Roman Vershynin. 2018. High-Dimensional Probability: An Introduction with
     Applications in Data Science. Cambridge University Press. https://doi.org/10.
     1017/9781108231596
[70] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. 2016. Learning to Hash
     for Indexing Big Data - A Survey. Proc. IEEE 104, 1 (2016), 34â€“57. https://doi.
     org/10.1109/JPROC.2015.2487976
[71] Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xi-
     angyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing
     Yuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang,
     Yihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, and Charles Xie. 2021. Milvus: A
     Purpose-Built Vector Data Management System. In Proceedings of the 2021 In-
     ternational Conference on Management of Data (Virtual Event, China) (SIGMOD
                                                                                         14
APPENDIX                                                                                        Proof. We first prove that âˆ€ð‘¡ > 0, y âˆˆ G, âˆ¥oâ€² âˆ’ð‘¡yâˆ¥ 2 â‰¥ 1âˆ’ âŸ¨o, oÌ„âŸ© 2
                                                                                             as follows.
A THE PROOF OF LEMMA 3.1
    Proof. Note that                                                                           âˆ¥oâ€² âˆ’ ð‘¡yâˆ¥ 2 = âˆ¥oâ€² âˆ¥ 2 âˆ’ 2ð‘¡ oâ€², y + ð‘¡ 2 âˆ¥yâˆ¥ 2                          (26)
                                                                                                                           2       2          â€²      2     2
                                                                                                            = 1 âˆ’ âŸ¨o, oÌ„âŸ© + âŸ¨o, oÌ„âŸ© âˆ’ 2ð‘¡ o , y + ð‘¡ âˆ¥yâˆ¥               (27)
                        âˆ¥y âˆ’ ð‘¡oâ€² âˆ¥ 2 = âˆ¥yâˆ¥ 2 + ð‘¡ 2 âˆ’ 2ð‘¡ oâ€², y                    (15)
                                                                                                                                    yÌ„ 2
                                                                                                                                       
                        âˆ¥ yÌ„ âˆ’ ð‘¡oâ€² âˆ¥ 2 = âˆ¥ yÌ„âˆ¥ 2 + ð‘¡ 2 âˆ’ 2ð‘¡ oâ€², yÌ„               (16)                       = 1 âˆ’ âŸ¨o, oÌ„âŸ© 2 + oâ€²,           âˆ’ 2ð‘¡ oâ€², y + ð‘¡ 2 âˆ¥yâˆ¥ 2   (28)
                                                                                                                                  âˆ¥ yÌ„âˆ¥
                                                                                                                                    y 2
                                                                                                                                       
Then to prove that âˆƒð‘¡ > 0 such that âˆ¥yâˆ’ð‘¡oâ€² âˆ¥ 2 â‰¥ âˆ¥ yÌ„âˆ’ð‘¡oâ€² âˆ¥ 2, âˆ€y âˆˆ G,
                                                                                                            â‰¥ 1 âˆ’ âŸ¨o, oÌ„âŸ© 2 + oâ€²,           âˆ’ 2ð‘¡ oâ€², y + ð‘¡ 2 âˆ¥yâˆ¥ 2   (29)
it suffices to prove that                                                                                                          âˆ¥yâˆ¥
                                                                                                                                                 2
                                                                                                                                      y
                    âˆ¥yâˆ¥ 2 âˆ’ âˆ¥ yÌ„âˆ¥ 2 âˆ’ 2ð‘¡ oâ€², y + 2ð‘¡ oâ€², yÌ„ â‰¥ 0                   (17)                       = 1 âˆ’ âŸ¨o, oÌ„âŸ© 2 + oâ€²,           âˆ’ ð‘¡ âˆ¥yâˆ¥                  (30)
                                                                                                                                    âˆ¥yâˆ¥
          âˆ¥ yÌ„âˆ¥ 2                                                                                           â‰¥ 1 âˆ’ âŸ¨o, oÌ„âŸ© 2                                          (31)
Let ð‘¡ = âŸ¨oâ€² ,yÌ„âŸ© . We derive from the left hand side as follows.
                                                                                             where (27) is because oâ€² is a unit vector. (28) applies ð‘ƒ âˆ’1
                          2        2                   â€²            â€²                        to both sides of the third term in (27). (29) is because yÌ„ =
                     âˆ¥yâˆ¥ âˆ’ âˆ¥ yÌ„âˆ¥ âˆ’ 2ð‘¡ o , y + 2ð‘¡ o , yÌ„                          (18)
                                                                                             arg maxyâˆˆ G âŸ¨y/âˆ¥yâˆ¥, oâ€² âŸ© and G is symmetric, i.e., âˆ€y âˆˆ G, we have
                                                  âˆ¥ yÌ„âˆ¥ 2
                    =âˆ¥yâˆ¥ 2 âˆ’ âˆ¥ yÌ„âˆ¥ 2 âˆ’ 2                    Â· oâ€², y + 2âˆ¥ yÌ„âˆ¥ 2
                                                                                                                                                   D          E
                                                                                                                                                           yÌ„
                                                 âŸ¨oâ€², yÌ„âŸ©
                                                                                 (19)        âˆ’y âˆˆ G. Then we note that when y = yÌ„ and ð‘¡ = âˆ¥ yÌ„âˆ¥1    oâ€², âˆ¥ yÌ„âˆ¥ , the
                                          âˆ¥ yÌ„âˆ¥ 2                                            equality holds. Taking the square root of both sides finishes the
                    =âˆ¥yâˆ¥ 2 + âˆ¥ yÌ„âˆ¥ 2 âˆ’ 2             Â· oâ€² , y                    (20)        proof.                                                                â–¡
                                         âŸ¨oâ€², yÌ„âŸ©
                                                                  
                                          âˆ¥ yÌ„âˆ¥âˆ¥yâˆ¥              y
                    =âˆ¥yâˆ¥ 2 + âˆ¥ yÌ„âˆ¥ 2 âˆ’ 2 D            E Â· oâ€² ,                   (21)           Lemma B.3. For any ð¿ > 0, we have
                                                  yÌ„
                                           oâ€², âˆ¥ yÌ„âˆ¥           âˆ¥yâˆ¥                                    âˆšï¸ƒ
                                                                                                                          ð¿   ð‘1      ð‘   
                                                                                                                                         0
                                                                                                    P     1 âˆ’ âŸ¨o, oÌ„âŸ© 2 > ðµ + âˆš Â· exp âˆ’ ð¿ 2 < ð›¿                      (32)
                â‰¥âˆ¥yâˆ¥ 2 + âˆ¥ yÌ„âˆ¥ 2 âˆ’ 2âˆ¥ yÌ„âˆ¥âˆ¥yâˆ¥                                     (22)                                    2     ð›¿        2
                    = (âˆ¥ yÌ„âˆ¥ âˆ’ âˆ¥yâˆ¥) 2 â‰¥ 0                                        (23)        where ð‘ 0 and ð‘ 1 are absolute constants.
                                                                                                                                       
                                       âˆ¥ yÌ„âˆ¥ 2
where (19) plugs in ð‘¡ = âŸ¨oâ€² ,yÌ„âŸ© . (20) and (21) rewrite the form of                            Proof. Let ð‘“ (ð‘¥) = sgn(ð‘¥) Â· min |ð‘¥ |, âˆšð¿ where sgn(ð‘¥) = âˆ’1
                                                                                                                                              ð·
                                                                                             if ð‘¥ < 0 and sgn(ð‘¥) = +1 if ð‘¥ â‰¥ 0. We let ð‘“ (oâ€² ) denote the vector
                                                  D       E
                                                    y
(19). (22) applies the fact that yÌ„ = arg maxyâˆˆ G âˆ¥yâˆ¥ , oâ€² , i.e., âˆ€y âˆˆ
   D
       yÌ„
             E D
                    y
                          E     D
                                    yÌ„
                                          E                                                  whose  ð‘–th coordinate is ð‘“ (oâ€² [ð‘–]). Next we start the derivation from
G, âˆ¥ yÌ„âˆ¥ , oâ€² â‰¥ âˆ¥yâˆ¥ , oâ€² and âˆ¥ yÌ„âˆ¥ , oâ€² > 0. (23) finishes the proof.
                                                                                             âˆšï¸ƒ
                                                                                               1 âˆ’ âŸ¨o, oÌ„âŸ© 2 as follows.
                                                                      â–¡
                                                                                                      âˆšï¸ƒ
B     THE PROOF OF THEOREM 3.2                                                                          1 âˆ’ âŸ¨o, oÌ„âŸ© 2 =    min âˆ¥oâ€² âˆ’ ð‘¡yâˆ¥                             (33)
                                                                                                                        ð‘¡ >0,yâˆˆ G
To make the paper self-contained, we restate the tail bound of the                                                    â‰¤ min oâ€² âˆ’ ð‘¡ 0 y                               (34)
coordinates of the random vector which follows uniform distribu-                                                        yâˆˆ G
tion on the unit sphere Sð· âˆ’1 in the ð·-dimensional space.                                                             = min oâ€² âˆ’ ð‘“ (oâ€² ) + ð‘“ (oâ€² ) âˆ’ ð‘¡ 0 y           (35)
                                                                                                                        yâˆˆ G
   Lemma B.1. ([69]) For a random vector x = (x[1], x[2], ..., x[ð·])                                                  â‰¤ min oâ€² âˆ’ ð‘“ (oâ€² ) + ð‘“ (oâ€² ) âˆ’ ð‘¡ 0 y           (36)
which follows the uniform distribution on the unit sphere Sð· âˆ’1 in                                                      yâˆˆ G
the ð·-dimensional space, the tail bound of its coordinates is given as                                                = oâ€² âˆ’ ð‘“ (oâ€² ) + min ð‘“ (oâ€² ) âˆ’ ð‘¡ 0 y           (37)
                                                                                                                                       yâˆˆ G
                                                                    
                                    ð‘¡                                                        Here ð‘¡ 0 := ðµâˆ’1ð¿âˆš . (33) applies Lemma B.2. (34) relaxes the min-
                        P |x[ð‘–]| > âˆš                   â‰¤ 2 exp âˆ’ð‘ 0ð‘¡ 2           (24)                     2    ð·
                                     ð·                                                       imum over ð‘¡ > 0 to a specific ð‘¡ 0 . (35) and (36) apply triangleâ€™s
where ð‘ 0 is a constant, ð‘– = 1, 2, ..., ð·.                                                   inequality. (37) holds because âˆ¥oâ€² âˆ’ ð‘“ (oâ€² )âˆ¥ is independent of y. We
                                                                                             next analyze these two terms in (37) separately. Let us first analyze
  We next prove Theorem 3.2 by first proving Lemma B.2 and                                   the expected value of âˆ¥oâ€² âˆ’ ð‘“ (oâ€² ) âˆ¥ 2 as follows.
Lemma B.3.                                                                                       h               i
                                                                                                                      "ð·                          #
                                                                                                                       âˆ‘ï¸
                                                                                               E oâ€² âˆ’ ð‘“ (oâ€² )             |oâ€² [ð‘–] âˆ’ ð‘“ (oâ€² [ð‘–])| 2
                                                                                                               2
                                                                                                                   =E                                          (38)
   Lemma B.2. Let o be a unit vector and oÌ„ be its quantized vector in                                                     ð‘–=1
Gð‘Ÿ , oâ€² = ð‘ƒ âˆ’1 o. Then
                                                                                                                   =ð· Â· E |oâ€² [1] âˆ’ ð‘“ (oâ€² [1])| 2
                                                                                                                                                 
                                                                                                                                                               (39)
                         âˆšï¸ƒ                                                                                             âˆ« +âˆž
                           1 âˆ’ âŸ¨o, oÌ„âŸ© 2 =             min âˆ¥oâ€² âˆ’ ð‘¡yâˆ¥                                                          P |oâ€² [1] âˆ’ ð‘“ (oâ€² [1])| 2 > ð‘¡ dð‘¡ (40)
                                                                                                                                
                                                                                 (25)                              =ð· Â·
                                                  ð‘¡ >0,yâˆˆ G                                                                    0
                                                                                        15
                     âˆ« +âˆž 
                                             âˆš
                                                         
                                                     ð¿
                 =ð· Â·           P |oâ€² [1]| > ð‘¡ + âˆš         dð‘¡            (41)          Proof. For simplifying the notations, let us define the following
                       0                              ð·                              events.
                     âˆ« +âˆž                  âˆš                                                                          âˆšï¸„                 âˆšï¸„
                <ð· Â·            2 exp âˆ’ð‘ 0 ( ð·ð‘¡ + ð¿) 2 dð‘¡                (42)                     âŸ¨oÌ„, qâŸ©                  1 âˆ’ âŸ¨oÌ„, oâŸ© 2       log(1/ð›¿)
                                                                                             ð¸ð´ :           âˆ’ âŸ¨o, qâŸ© â‰¤                   Â·                (53)
                       0
                                âˆ« +âˆž                                                              âŸ¨oÌ„, oâŸ©                    âŸ¨oÌ„, oâŸ© 2        ð‘ 0 (ð· âˆ’ 1)
                       âˆ’ð‘ 0 ð¿ 2
                â‰¤ð· Â· ð‘’                 2 exp (âˆ’ð‘ 0 ð·ð‘¡) dð‘¡
                                                                                                 âˆšï¸„
                                                                         (43)
                                                                                                    1 âˆ’ âŸ¨oÌ„, oâŸ© 2     ð¿     ð‘1           ð‘
                                                                                                                                            0
                                                                                                                                                   
                                        0                                                    ð¸ðµ :              2
                                                                                                                   â‰¤ ðµ + âˆš Â· exp âˆ’ ð¿ 2                    (54)
                                        2   2                                                          âŸ¨oÌ„, oâŸ©       2                      2
                 =ð· Â· ð‘’ âˆ’ð‘ 0 ð¿                Â· ð‘’ âˆ’ð‘ 0 ð¿
                                   2                     2                                                                    ð›¿
                                          =                              (44)
                                       ð‘0ð· ð‘0                                        In particular, to provide an upper bound for the error of the estima-
                                                                                     tor, we target to prove that
(39) is because the linearity of expectation and the fact that oâ€² [ð‘–]â€™s
are identically distributed to each other. (40) is an elementary                                            P {ð¸ð´ and ð¸ðµ } â‰¥ 1 âˆ’ 2ð›¿                      (55)
property of non-negative random variables. (41) can be verified                      Based on the union bound, it suffices to prove that
by the definition
         âˆš           of ð‘“ (ð‘¥). (42) applies Lemma B.1. (43) holds be-                                    P {Â¬ð¸ð´ } â‰¤ ð›¿ and P {Â¬ð¸ðµ } â‰¤ ð›¿                   (56)
cause ( ð·ð‘¡ + ð¿) 2 > ð·ð‘¡ + ð¿ 2 . (44) is by elementary calculus. Next
by applying Markovâ€™s inequality [69], we derive the tail bound of                    Note that P {Â¬ð¸ð´ } â‰¤ ð›¿ holds due to Lemma 2.1 and P {Â¬ð¸ðµ } â‰¤ ð›¿
âˆ¥oâ€² âˆ’ ð‘“ (oâ€² )âˆ¥ 2 as follows.                                                         holds
                                                                                        âˆšï¸ƒ because of Lemma B.3 (Lemma B.3 provides an upper bound
                                                                                    for 1 âˆ’ âŸ¨o, oÌ„âŸ© 2 and implies that âŸ¨o, oÌ„âŸ© is lower bounded by 1/2).
                                                             
                                                                                     Thus, with the probability of at least 1 âˆ’ 2ð›¿, we have
                                         2            
                 P oâ€² âˆ’ ð‘“ (oâ€² ) â‰¥
                                   2
                                              Â· exp âˆ’ð‘ 0 ð¿ 2             (45)
                                       ð‘ 0ð›¿                                                                                         
                                                                                                                                          âˆšï¸„
                                                                                        âŸ¨oÌ„, qâŸ©               ð¿   ð‘1        ð‘
                                                                                                                                 0 2          log(1/ð›¿)
                 ð‘ 0ð›¿                 h                  i                                    âˆ’ âŸ¨o, qâŸ© â‰¤ ðµ + âˆš Â· exp âˆ’ ð¿              Â·                (57)
                      Â· exp ð‘ 0 ð¿ 2 Â· E oâ€² âˆ’ ð‘“ (oâ€² )
                                                         2
               â‰¤                                                         (46)           âŸ¨oÌ„, oâŸ©              2     ð›¿            2            ð‘ 0 (ð· âˆ’ 1)
                   2
                 ð‘ 0ð›¿              2                                               To bound the right hand side by ðœ–, it suffices to let
                                          Â· ð‘’ âˆ’ð‘ 0 ð¿ = ð›¿
                                                    2
               <      Â· exp ð‘ 0 ð¿ 2 Â·                                    (47)                                                           
                   2                  ð‘0                                                                          1 (1/ð›¿) Â· log(1/ð›¿)
                                                                                                       ð¿ =Î˜ log      Â·                                   (58)
                                                                                                                  ð·           ðœ–2
Then we analyze the upper bound of the second term in (37). Note                                                                        
                                                                                                                  1 log(1/ð›¿)
that based on the definition of ð‘“ (ð‘¥), every coordinate of ð‘“ (oâ€² ) is                                  ðµ =Î˜ log      Â·             + log ð¿               (59)
within the range of [âˆ’ âˆšð¿ , + âˆšð¿ ]. Let us choose y from the set of                                               ð·      ðœ–2
                               ð·              ð·                                                                                
                                                                                                                  1 log(1/ð›¿)
grids G such that every coordinate ð‘¡ 0 y[ð‘–] is closest to ð‘“ (oâ€² [ð‘–]).                                    =Î˜ log      Â·                                   (60)
                         1 and y[ð‘–] ranges from (âˆ’2ðµâˆ’1 + 1 ) to
Recall that ð‘¡ 0 = âˆšð¿ Â· 2ðµâˆ’1                                                                                       ð·      ðœ–2
                                                              2
                        ð·                                                                                                                                  â–¡
(+2ðµâˆ’1 âˆ’ 12 ). Then for every dimension ð‘–, we have |ð‘“ (oâ€² [ð‘–]) âˆ’
ð‘¡ 0 y[ð‘–] | â‰¤ 21ðµ âˆšð¿ . Thus, we have                                                  C    USING RABITQ WITH >32X COMPRESSION
                ð·
                                                                                          RATE
                                                  ð¿
                            min ð‘“ (oâ€² ) âˆ’ ð‘¡ 0 y â‰¤ ðµ                      (48)        Let ð‘ƒð‘‘ be the random orthogonal matrix which projects a ð·-
                            yâˆˆ G                 2                                   dimensional vector to ð‘‘-dimensional space. We present the details
                                                                                     of the estimation for âˆ¥oð‘Ÿ âˆ’ qð‘Ÿ âˆ¥ 2 as follows.
Based on the analysis above, we prove (32) as follows.
                                                                                          âˆ¥oð‘Ÿ âˆ’ qð‘Ÿ âˆ¥ 2                                                   (61)
                                                                                                   2             2
                                                                                         =âˆ¥oð‘Ÿ âˆ’ câˆ¥ + âˆ¥qð‘Ÿ âˆ’ câˆ¥ âˆ’ 2 Â· âŸ¨oð‘Ÿ âˆ’ c, qð‘Ÿ âˆ’ câŸ©
                                         âˆšï¸„
        ï£±                                  2 ï£¼                                                                                                         (62)
        ï£² âˆšï¸ƒ                     2ð‘’ âˆ’ð‘ 0 ð¿ ï£´
        ï£´                                    ï£´
        ï£´              ð¿                     ï£½
    P 1 âˆ’ âŸ¨o, oÌ„âŸ© 2 > ðµ +                                                (49)                                       2ð·
    ï£´
    ï£´                 2              ð‘ 0ð›¿ ï£´  ï£´                                           â‰ˆâˆ¥oð‘Ÿ âˆ’ câˆ¥ 2 + âˆ¥qð‘Ÿ âˆ’ câˆ¥ 2 âˆ’    Â· âŸ¨ð‘ƒð‘‘ (oð‘Ÿ âˆ’ c), ð‘ƒð‘‘ (qð‘Ÿ âˆ’ c)âŸ©    (63)
    ï£³                                        ï£¾                                                                       ð‘‘
                                                       âˆšï¸„                                                           2ð·
                                                                                         =âˆ¥oð‘Ÿ âˆ’ câˆ¥ 2 + âˆ¥qð‘Ÿ âˆ’ câˆ¥ 2 âˆ’    Â· âˆ¥ð‘ƒð‘‘ (oð‘Ÿ âˆ’ c)âˆ¥ Â· âˆ¥ð‘ƒð‘‘ (qð‘Ÿ âˆ’ c) âˆ¥
    ï£±                                                              2 ï£¼
                                                          2ð‘’ âˆ’ð‘ 0 ð¿ ï£´
    ï£´                                                                ï£´
    ï£² â€²
    ï£´          â€²                  â€²                 ð¿                ï£½
  â‰¤P o âˆ’ ð‘“ (o ) + min ð‘“ (o ) âˆ’ ð‘¡ 0 y > ðµ +                               (50)                                        ð‘‘
    ï£´
    ï£´                yâˆˆ G                          2        ð‘ 0ð›¿ ï£´   ï£´
                                                                                            
                                                                                               ð‘ƒð‘‘ (oð‘Ÿ âˆ’ c)    ð‘ƒ (qð‘Ÿ âˆ’ c)
                                                                                                                           
    ï£³                                                                ï£¾                    Â·                , ð‘‘                                         (64)
    ï£±                                  âˆšï¸„
                                                   2 ï£¼
                                                                                              âˆ¥ð‘ƒð‘‘ (oð‘Ÿ âˆ’ c)âˆ¥ âˆ¥ð‘ƒð‘‘ (qð‘Ÿ âˆ’ c)âˆ¥
    ï£´
                      ð¿        ð¿             âˆ’ð‘
                                          2ð‘’ 0   ð¿   ï£´
  â‰¤P o âˆ’ ð‘“ (oâ€² ) + ðµ > ðµ +
    ï£² â€²
    ï£´                                                ï£½
                                                     ï£´
                                                                         (51)            â‰ˆâˆ¥oð‘Ÿ âˆ’ câˆ¥ 2 + âˆ¥qð‘Ÿ âˆ’ câˆ¥ 2 âˆ’ 2 Â· âˆ¥oð‘Ÿ âˆ’ câˆ¥ Â· âˆ¥qð‘Ÿ âˆ’ câˆ¥
    ï£´
    ï£´                2       2               ð‘ 0ð›¿ ï£´  ï£´                                                                     
    ï£³                                                ï£¾                                         ð‘ƒð‘‘ (oð‘Ÿ âˆ’ c)    ð‘ƒð‘‘ (qð‘Ÿ âˆ’ c)
                     âˆšï¸„                                                                   Â·                ,                                             (65)
    ï£±
    ï£´                      âˆ’ð‘
                         2ð‘’ 0 ï£´  ð¿
                                     ï£¼
                                   2 ï£´                                                        âˆ¥ð‘ƒð‘‘ (oð‘Ÿ âˆ’ c)âˆ¥ âˆ¥ð‘ƒð‘‘ (qð‘Ÿ âˆ’ c)âˆ¥
  =P oâ€² âˆ’ ð‘“ (oâ€² ) >
    ï£²
    ï£´                                ï£½
                                        â‰¤ð›¿                               (52)        Here  (63) and (65) are based
    ï£´                      ð‘ 0 ð›¿     ï£´                                               D                            E on Johnson-Lindenstrauss Lemma [39].
                                                                                        ð‘ƒð‘‘ (oð‘Ÿ âˆ’c)    ð‘ƒð‘‘ (qð‘Ÿ âˆ’c)
    ï£´                                ï£´
                                                                                       âˆ¥ð‘ƒð‘‘ (oð‘Ÿ âˆ’c) âˆ¥ âˆ¥ð‘ƒð‘‘ (qð‘Ÿ âˆ’c) âˆ¥ is the inner product between two unit
                                                                                                    ,
    ï£³                                ï£¾

where (50) applies (37). (51) applies (48). (52) applies (47).             â–¡         vectors in ð‘‘-dimensional space, whose estimation can be realized
                                                                                     with RaBitQ.

   Based on the lemmas above, we prove Theorem 3.2 as follows.
                                                                                16
